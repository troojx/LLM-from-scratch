{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22cea02",
   "metadata": {},
   "source": [
    "# 第五章：在无标签数据上进行预训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dd07e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "numpy version: 1.26.3\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1+cu121\n",
      "tensorflow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6577c",
   "metadata": {},
   "source": [
    "- 在本章中，我们将实现一个训练循环 training loop 以及基本的模型评估代码，以便对大型语言模型（LLM）进行预训练。最后，我们还将从 OpenAI 加载公开的预训练权重并将其导入到我们的模型中。\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/chapter-overview.webp\" width=500px>\n",
    "- 下面是这张的主题\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model--0.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9b9a9",
   "metadata": {},
   "source": [
    "## 5.1 生成式文本模型的评估\n",
    "- 首先，我们简要回顾使用上一章中的代码初始化 GPT 模型。然后，我们讨论用于**评估 LLM 的基本评价指标**，最后将这些指标应用到训练和验证数据集上。\n",
    "\n",
    "### 5.1.1 使用 GPT 生成文本\n",
    "- 我们使用上一章的代码初始化一个 GPT 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5bc7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8a358",
   "metadata": {},
   "source": [
    "- 这里将 dropout 设置为 0.1，但如今较为常见的是训练 LLM 时不使用 dropout。\n",
    "- 此外，现代 LLM 也不再在 nn.Linear 层中添加偏置向量，特别是用于 query、key 和 value 矩阵时（与早期 GPT 模型不同），可以通过设置 \"qkv_bias\": False 来实现这一点。\n",
    "\n",
    "- 我们将上下文长度 context_length 降至 256 个 token，以降低模型训练的计算资源需求，而原始的 124M 参数 GPT-2 模型使用的是 1024 个 token。这种简化可以让更多读者在笔记本电脑上跟随和运行代码示例。当然，也可以将 context_length 增加到 1024（无需更改代码）。之后，我们还会加载一个具有 1024 个 context_length 的预训练权重模型。\n",
    "\n",
    "- 接下来，我们使用上一章的 generate_text_simple 函数生成文本。此外，我们定义了两个便捷函数 text_to_token_ids 和 token_ids_to_text，用于在 token 表示和文本表示之间进行转换，这两个函数在本章中会被频繁使用。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-process.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775e2e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(  #生成新文本\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1256c6",
   "metadata": {},
   "source": [
    "- 正如上方输出所见，模型尚未产生良好的文本，因为还未经过训练。\n",
    "- 那么，如何用数值的形式来衡量和捕捉“良好的文本”，以便在训练过程中进行跟踪呢？\n",
    "- 接下来的小节将介绍用于计算生成输出的损失loss指标的度量方式，以帮助我们衡量训练进展。接下来的 LLM 微调章节还会介绍更多方法来衡量模型质量。\n",
    "\n",
    "\n",
    "### 5.1.2 计算文本生成损失：交叉熵cross-entropy和困惑度perplexity\n",
    "- 假设我们有一个 inputs 张量，包含 2 个训练样本（行）的 token ID。\n",
    "- 对应于 inputs，targets 包含了我们希望模型生成的目标 token ID。注意，targets 是 inputs 向右偏移一个位置后的结果，正如第 2 章实现数据加载器时所解释的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d512769",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb9008",
   "metadata": {},
   "source": [
    "- 将 inputs 输入到模型中，我们得到每个输入样本的 logits 向量，这些向量包含 3 个 token。每个 token 是一个 50,257 维的向量，对应于词汇表的大小。通过应用 softmax 函数，我们可以将 logits 张量转换为同维度的概率分数张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e056a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "#开启了一个上下文管理器，在其中不会计算梯度。用于测试或评估模型，以节省内存和提高推理效率，因为在模型评估时不需要反向传播计算。\n",
    "with torch.no_grad():\n",
    "     # 对输入 `inputs` 进行前向传播，计算每个 token 在词汇表中所有词的分数\n",
    "        #logits 是一个三维张量，表示每个 token 对词汇表中所有词的分数。\n",
    "    logits = model(inputs)   \n",
    "\n",
    "# 对 logits 应用 softmax，将分数转换为概率分布\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "# 预计输出的形状为 (batch_size, num_tokens, vocab_size)\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a21ff",
   "metadata": {},
   "source": [
    "- 下图使用一个非常小的词汇表进行示例，展示了如何将概率分数转换回文本，这是上一章结尾讨论的内容。\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-to-text.webp\" width=500px>\n",
    "- 正如上一章所述，我们可以应用 argmax 函数，将概率分数转换为预测的 token ID。\n",
    "- 上方的 softmax 函数为每个 token 生成一个 50,257 维向量，而 argmax 函数返回该向量中最高概率分数的位置，这就是给定 token 的预测 token ID。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb799eca",
   "metadata": {},
   "source": [
    "- 因为有两个输入batch，每个有3个token，we obtain 2 by 3 predicted token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c2adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4dbae0",
   "metadata": {},
   "source": [
    "- 如果我们将这些预测的tokens解码，会发现它们与模型应预测的目标tokens有很大差异。例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de730df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf52be",
   "metadata": {},
   "source": [
    "- 这是因为模型尚未训练。为了训练模型，我们需要了解 how far it is away from the correct predictions (targets)\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=500px>\n",
    "- token可能性对应着目标index："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde15abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d692fbea",
   "metadata": {},
   "source": [
    "- 接下来，我们关注目标索引对应的tokens概率。目标是将这些概率值尽量增大，尽可能接近1。在数学优化中，通常更容易最大化 **概率分数的对数logarithm of the probability score** ，而不是直接最大化概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f81f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38bf5f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "#计算平均对数概率\n",
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed8ab6",
   "metadata": {},
   "source": [
    "- 目标是通过优化模型权重，让average log probability越大越好\n",
    "- 因为取了log，最大的possibility是0，我们目前离0 很远\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed49f7b",
   "metadata": {},
   "source": [
    "- 在深度学习中，我们不直接最大化平均对数概率，而是最小化负的平均对数概率。例如，不是最大化-10.7722让他接近0，而是最小化10.7722让他接近0\n",
    "- 在这种情况下，取负后的对数值（如10.7722）被称为**交叉熵损失cross-entropy**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1cf1004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1   #变成正数\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651386c4",
   "metadata": {},
   "source": [
    "- pytorch的`cross_entropy` function可以做前面的步骤\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/cross-entropy.webp?123\" width=400px>\n",
    "- 在使用`cross_entropy` function之前，检查logits和targets的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3483498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a53dc0",
   "metadata": {},
   "source": [
    "- For the cross_entropy function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "450448be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#如何将 logits 和 targets 张量拉平成适合用于 PyTorch cross_entropy 损失函数的格式\n",
    "\n",
    "#logits 是模型生成的输出张量，其中包含了每个 token 在词汇表中每个单词的分数（未经过 softmax 转换的概率）。\n",
    "#该张量的形状通常为 (batch_size, num_tokens, vocab_size)。\n",
    "#targets 是目标张量，包含了模型期望生成的每个 token 的真实词汇索引，\n",
    "#其形状通常为 (batch_size, num_tokens)。\n",
    "logits_flat = logits.flatten(0, 1)       # 将 logits 的第 0 和 1 维展平\n",
    "targets_flat = targets.flatten()          # 将 targets 展平为 1D 张量\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bffb2",
   "metadata": {},
   "source": [
    "- 需要注意的是，目标tokens是指目标的token ID，同时也代表logits张量中需要最大化的索引位置。\n",
    "- PyTorch中的cross_entropy函数会自动处理这些目标索引上进行softmax和对数概率计算的细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f454ec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    " #                                           使用拉平的向量\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)   #表示当前的模型离目标还有多远"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8ea51",
   "metadata": {},
   "source": [
    "\n",
    "- 与交叉熵损失相关的概念是困惑度（Perplexity）。**Perplexity只是交叉熵损失的指数形式**，被认为更具解释性，它表示模型在每一步不确定的 有效词汇大小 effective vocabulary size that the model is uncertain about at each step（下面的例子中，是48725个单词或tokens）。\n",
    "- 换句话说，perplexity衡量模型预测的概率分布与数据集中实际分布的匹配度。类似于损失值，较低的困惑度表明模型预测更接近真实分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e8a1abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4362d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  5.1.3 计算训练集和验证集的损失training and validation set losses\n",
    "- 在训练LLM时，我们使用了相对较小的数据集（实际上只是一个短篇故事）。这样做的原因包括：\n",
    "    - 在没有GPU的情况下，可以在笔记本电脑上快速运行代码示例。\n",
    "    - 训练时间相对较短（几分钟，而不是数周），适合教育用途。\n",
    "    - 我们使用了公有领域的文本，方便直接包含在GitHub仓库中，而不会侵犯使用权或使仓库体积过大。\n",
    "\n",
    "- 例如，Llama 2 7B模型在A100 GPU上训练2万亿tokens需要184,320 GPU小时，而在AWS上，一个包含8个A100的服务器每小时约30美元。粗略计算，训练此LLM的成本将为184,320 / 8 * $30 = $690,000。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad7d8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b82a779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "#检查一下正不正确\n",
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67725589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d8ae997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)    #总文本长度\n",
    "total_tokens = len(tokenizer.encode(text_data))    #解码后的token数量\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09430b75",
   "metadata": {},
   "source": [
    "- 由于训练一个大型语言模型 (LLM) 的文本仅有 5,145 个 token，非常短，这只是为了教学目的（稍后我们还会加载预训练的权重）。\n",
    "- 接下来，我们将数据集分为训练集和验证集 training and a validation set ，并使用第 2 章的数据加载器来准备 LLM 训练所需的批次 batches数据。\n",
    "- 为了便于可视化，图中假设了 max_length=6，但在训练加载器中，我们将 max_length 设置为 LLM 所支持的上下文长度。\n",
    "    - 图中仅显示了输入 tokens，为了简化表示。\n",
    "    - 由于我们训练 LLM 去预测文本中的下一个词，因此目标数据与输入数据看起来是相同的，只是目标数据向后偏移了一个位置。\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd6073dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio \n",
    "train_ratio = 0.90  #用90%的文本用于训练，10%用于验证\n",
    "split_idx = int(train_ratio * len(text_data))   #分开training和validation的文本\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "#保证结果可复现性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "#训练数据集\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    #滑动窗口的步幅与上下文长度相同。数据加载器在从数据集中抽取样本时会以 context_length 为步幅滑动\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    #在分割数据集时丢弃不能完全匹配 batch_size 的最后一个批次，以保证每个批次的大小一致；\n",
    "    drop_last=True,\n",
    "    #随机打乱样本，确保训练数据的随机性；\n",
    "    shuffle=True,\n",
    "    #此处设置为 0 表示在主进程中加载数据，不创建额外的子进程。\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "#验证数据集\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    #确保验证数据按顺序加载，从而更稳定地评估模型。\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ae54209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 检验训练和检验文本的长度是否足够\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2b105",
   "metadata": {},
   "source": [
    "- 我们使用了较小的批次大小，以减少对计算资源的需求，因为数据集本身就很小。\n",
    "- 例如，Llama 2 7B 的训练批次大小为 1024。\n",
    "- 这里可以进行一个可选的检查，以确保数据被正确加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34101841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "#可选的检查\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27650920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "# 另一个可选的检查：确保 token 尺寸在预期范围内。\n",
    "\n",
    "train_tokens = 0\n",
    "#遍历 train_loader 中的每个批次，input_batch 是模型的输入 token，target_batch 是对应的目标 token。\n",
    "for input_batch, target_batch in train_loader:\n",
    "    #numel() 是 PyTorch 的一个方法，返回张量中的元素数量，即 input_batch 中所有 token 的总数。\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf27c95",
   "metadata": {},
   "source": [
    "- 接下来，我们实现了一个实用函数来计算给定批次的交叉熵损失cross-entropy loss。\n",
    "- 除此之外，我们还实现了第二个实用函数，用于计算数据加载器中用户指定的批次数的损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b4e6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-entropy\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "# 计算平均损失\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # if num_batches 超过 the number of batches in the data loader\n",
    "        # 减少batches数量 to match batches总数 in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))  #取最小值\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            #alc_loss_batch 内部会调用 cross-entropy loss 来计算单个批次的损失。\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a702c2",
   "metadata": {},
   "source": [
    "- 如果您的计算机配备支持 CUDA 的 GPU，LLM 将在 GPU 上进行训练，而无需对代码进行任何更改。通过设置 device，我们可以确保数据加载到与 LLM 模型相同的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c0329c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93efe25",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-1.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67761983",
   "metadata": {},
   "source": [
    "## 5.2 Training an LLM\n",
    "- 在本节中，我们最终实现训练 LLM 的代码。\n",
    "- 我们将重点放在一个简单的训练函数上（如果你对更高级的技术感兴趣，比如学习率预热、余弦退火和梯度裁剪， learning rate warmup, cosine annealing, and gradient clipping ，请参阅附录 D）。\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4eab75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_freq: 评估频率，每隔多少步进行评估。\n",
    "# eval_iter: 评估时使用的迭代次数。\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    #设置训练损失列表、验证损失列表、tokens统计等变量\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            #重置上一次小批次的梯度。\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            #计算小批次的损失并反向传播更新权重。\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()  #累加\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            #到了该评估的时候了\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model( \n",
    "                    model, train_loader, val_loader, device, eval_iter)  #评估时使用的迭代次数\n",
    "                train_losses.append(train_loss) \n",
    "                val_losses.append(val_loss)     #将评估损失添加到列表中。\n",
    "                track_tokens_seen.append(tokens_seen)             #记录已经处理的token数量\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "#在训练过程中定期评估模型的性能。\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():     #设置模型为评估模式，禁用梯度计算。\n",
    "        #调用 calc_loss_loader 函数计算训练和验证集的平均损失。\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "   #返回模型到训练模式并返回训练和验证损失。\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "#在每个epoch后生成并打印一段文本样例。\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()   #进入评估模式\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)  #解码\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )       #生成50个新token\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)     #解码转换成文本\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e7fbd",
   "metadata": {},
   "source": [
    "- 使用上述训练方法训练LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05398a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.820, Val loss 9.927\n",
      "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.338\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.624, Val loss 7.050\n",
      "Ep 2 (Step 000015): Train loss 6.049, Val loss 6.603\n",
      "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.559, Val loss 6.494\n",
      "Ep 3 (Step 000025): Train loss 5.428, Val loss 6.384\n",
      "Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 4.974, Val loss 6.285\n",
      "Ep 4 (Step 000035): Train loss 4.721, Val loss 6.303\n",
      "Every effort moves you of the picture.      \"I                \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss 4.058, Val loss 6.159\n",
      "Every effort moves you know the                          \"Oh, and the fact a little the latter the honour his pictures--and it's--I he had\n",
      "Ep 6 (Step 000045): Train loss 3.676, Val loss 6.174\n",
      "Ep 6 (Step 000050): Train loss 3.117, Val loss 6.148\n",
      "Every effort moves you know the fact, and pushed one of the to the fact of the last word.        \"Oh, and I was his pictures--I had the donkey. I had the donkey. \"I looked. \n",
      "Ep 7 (Step 000055): Train loss 3.026, Val loss 6.182\n",
      "Ep 7 (Step 000060): Train loss 2.281, Val loss 6.145\n",
      "Every effort moves you know,\" was not that the picture.  I-chairs forward. \"There: \"Yes, and!  \"I didn't say, and I was a little. \"I he was his pictures--because he was his pictures\n",
      "Ep 8 (Step 000065): Train loss 1.863, Val loss 6.164\n",
      "Ep 8 (Step 000070): Train loss 1.532, Val loss 6.259\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word. Gisburn's an awful simpleton, and Mrs. I remember getting off a prod, as once one had been the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.181, Val loss 6.249\n",
      "Ep 9 (Step 000080): Train loss 0.900, Val loss 6.266\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the honour being _mine_--because he's. The\n",
      "Ep 10 (Step 000085): Train loss 0.658, Val loss 6.369\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 0.35 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "#导入 time 模块，并使用 time.time() 记录训练开始时间，方便后续计算总的执行时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "#创建一个 AdamW 优化器并传入模型参数进行优化。\n",
    "#lr=0.0004 指定学习率为 0.0004。\n",
    "#weight_decay=0.1 用于 L2 正则化，有助于减少模型过拟合。\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "#设置训练的 epoch 数为 10，即完整遍历训练集 10 次。\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11cf636a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABX6UlEQVR4nO3dd3gUVdvA4d9u+qb3QgotkARCDxgCohApIgKiWPgUFOWVKmLBgghYEEREEFEs8PpKsQEiUgxI7y0UgdACCYEktHRS93x/bNiwUiSQsJvw3Nc1F7tnzsw8OyR59sycM0ejlFIIIYQQwiJpzR2AEEIIIa5PErUQQghhwSRRCyGEEBZMErUQQghhwSRRCyGEEBZMErUQQghhwSRRCyGEEBZMErUQQghhwSRRCyGEEBZMErUQ1cCJEyfQaDTEx8ebOxQhRAWTRC2EhdBoNDdcxowZY+4QhRBmYG3uAIQQBmfOnDG+/vHHHxk9ejQJCQnGMicnJ3OEJYQwM2lRC2Eh/Pz8jIurqysajcb43sfHh8mTJxMYGIidnR1NmjRh+fLl191XSUkJzz33HGFhYSQlJQHw22+/0axZM+zt7alduzZjx46luLjYuI1Go+Gbb76hZ8+e6HQ6QkNDWbx4sXH9xYsX6dOnD97e3jg4OBAaGsqsWbOuG8Mvv/xCZGQkDg4OeHp6EhsbS25urnH9N998Q3h4OPb29oSFhfHFF1+YbJ+cnEzv3r1xc3PDw8OD7t27c+LECeP6fv360aNHDyZNmoS/vz+enp4MHjyYoqKimz7nQlQJSghhcWbNmqVcXV2N7ydPnqxcXFzUvHnz1KFDh9Trr7+ubGxs1OHDh5VSSiUmJipA7d69W+Xn56uePXuqpk2bqvT0dKWUUuvWrVMuLi5q9uzZ6tixY+rPP/9UNWvWVGPGjDEeA1CBgYFq7ty56siRI2rYsGHKyclJnT9/Ximl1ODBg1WTJk3U9u3bVWJiooqLi1OLFy++ZvynT59W1tbWavLkySoxMVHt3btXTZ8+XWVnZyullPrhhx+Uv7+/+vXXX9Xx48fVr7/+qjw8PNTs2bOVUkoVFhaq8PBw9dxzz6m9e/eqAwcOqKeeekrVr19fFRQUKKWU6tu3r3JxcVEvvviiOnjwoPr999+VTqdTM2fOrNj/DCHMTBK1EBbon4k6ICBAffDBByZ1oqKi1KBBg5RSZYl6/fr1qkOHDqpNmzYqIyPDWLdDhw7qww8/NNn+f//7n/L39ze+B9SoUaOM73NychSgli1bppRSqlu3burZZ5+9qfh37typAHXixIlrrq9Tp46aO3euSdl7772noqOjjbHVr19f6fV64/qCggLl4OCgVqxYoZQyJOqQkBBVXFxsrPPYY4+pxx9//KZiFKKqkHvUQli4rKwsTp8+TUxMjEl5TEwMe/bsMSl78sknCQwM5K+//sLBwcFYvmfPHjZu3MgHH3xgLCspKSE/P5+8vDx0Oh0AjRo1Mq53dHTExcWF9PR0AAYOHEivXr3YtWsXHTt2pEePHrRu3fqaMTdu3JgOHToQGRlJp06d6NixI48++iju7u7k5uZy7Ngx+vfvzwsvvGDcpri4GFdXV2O8R48exdnZ2WS/+fn5HDt2zPi+QYMGWFlZGd/7+/uzb9++G5xNIaoeSdRCVCMPPvggP/zwA5s3b6Z9+/bG8pycHMaOHcsjjzxy1Tb29vbG1zY2NibrNBoNer0egC5dunDy5EmWLl1KXFwcHTp0YPDgwUyaNOmqfVpZWREXF8emTZv4888/mTZtGm+//TZbt241fin4+uuvadWq1VXbXY63efPmzJkz56p9e3t731S8QlQXkqiFsHAuLi4EBASwceNG2rVrZyzfuHEjLVu2NKk7cOBAGjZsyMMPP8wff/xhrN+sWTMSEhKoW7fubcXi7e1N37596du3L23btuW11167ZqIGQ9KMiYkhJiaG0aNHExISwsKFCxkxYgQBAQEcP36cPn36XHPbZs2a8eOPP+Lj44OLi8ttxSxEVSeJWogq4LXXXuPdd9+lTp06NGnShFmzZhEfH3/NFufQoUMpKSnhoYceYtmyZbRp04bRo0fz0EMPERwczKOPPopWq2XPnj3s37+f999//6ZiGD16NM2bN6dBgwYUFBSwZMkSwsPDr1l369atrFq1io4dO+Lj48PWrVs5e/assf7YsWMZNmwYrq6udO7cmYKCAnbs2MHFixcZMWIEffr04eOPP6Z79+6MGzeOwMBATp48yYIFC3j99dcJDAy89ZMpRBUjiVqIKmDYsGFkZmbyyiuvkJ6eTkREBIsXLyY0NPSa9YcPH45er+fBBx9k+fLldOrUiSVLljBu3DgmTJiAjY0NYWFhPP/88zcdg62tLW+++SYnTpzAwcGBtm3bMn/+/GvWdXFxYd26dUyZMoWsrCxCQkL45JNP6NKlCwDPP/88Op2Ojz/+mNdeew1HR0ciIyMZPnw4ADqdjnXr1jFy5EgeeeQRsrOzqVGjBh06dJAWtrjraJRSytxBCCGEEOLa5IEnQgghhAWTRC2EEEJYMEnUQgghhAWTRC2EEEJYMEnUQgghhAWTRC2EEEJYMEnU1zF9+nRq1qyJvb09rVq1Ytu2beYOySKsW7eObt26ERAQgEajYdGiRSbrlVKMHj0af39/HBwciI2N5ciRIyZ1Lly4QJ8+fXBxccHNzY3+/fuTk5NjUmfv3r20bdsWe3t7goKCmDhx4lWx/Pzzz4SFhWFvb09kZCRLly6t8M97J40fP56oqCicnZ3x8fGhR48eJvNRg+FZ14MHD8bT0xMnJyd69epFWlqaSZ2kpCS6du2KTqfDx8eH1157zWQ6S4A1a9bQrFkz7OzsqFu3LrNnz74qnur4OzBjxgwaNWqEi4sLLi4uREdHs2zZMuN6Ob8V66OPPkKj0RjHx4Oc41ti5klBLNL8+fOVra2t+u6779Tff/+tXnjhBeXm5qbS0tLMHZrZLV26VL399ttqwYIFClALFy40Wf/RRx8pV1dXtWjRIrVnzx718MMPq1q1aqlLly4Z63Tu3Fk1btxYbdmyRa1fv17VrVtXPfnkk8b1mZmZytfXV/Xp00ft379fzZs3Tzk4OKivvvrKWGfjxo3KyspKTZw4UR04cECNGjVK2djYqH379lX6OagsnTp1UrNmzVL79+9X8fHx6sEHH1TBwcEqJyfHWOfFF19UQUFBatWqVWrHjh3qnnvuUa1btzauLy4uVg0bNlSxsbFq9+7daunSpcrLy0u9+eabxjrHjx9XOp1OjRgxQh04cEBNmzZNWVlZqeXLlxvrVNffgcWLF6s//vhDHT58WCUkJKi33npL2djYqP379yul5PxWpG3btqmaNWuqRo0aqZdeeslYLue4/CRRX0PLli3V4MGDje9LSkpUQECAGj9+vBmjsjz/TNR6vV75+fmpjz/+2FiWkZGh7Ozs1Lx585RSSh04cEABavv27cY6y5YtUxqNRqWkpCillPriiy+Uu7u7cd5hpZQaOXKkql+/vvF97969VdeuXU3iadWqlfrPf/5ToZ/RnNLT0xWg1q5dq5QynEsbGxv1888/G+scPHhQAWrz5s1KKcMXKa1Wq1JTU411ZsyYoVxcXIzn8/XXX1cNGjQwOdbjjz+uOnXqZHx/N/0OuLu7q2+++UbObwXKzs5WoaGhKi4uTrVr186YqOUc3xq59P0PhYWF7Ny5k9jYWGOZVqslNjaWzZs3mzEyy5eYmEhqaqrJuXN1daVVq1bGc7d582bc3Nxo0aKFsU5sbCxarZatW7ca69x7773Y2toa63Tq1ImEhAQuXrxorHPlcS7XqU7/R5mZmQB4eHgAsHPnToqKikw+d1hYGMHBwSbnNzIyEl9fX2OdTp06kZWVxd9//22sc6Nzd7f8DpSUlDB//nxyc3OJjo6W81uBBg8eTNeuXa86D3KOb4086/sfzp07R0lJickPCYCvry+HDh0yU1RVQ2pqKsA1z93ldampqfj4+Jist7a2xsPDw6ROrVq1rtrH5XXu7u6kpqbe8DhVnV6vZ/jw4cTExNCwYUPA8NltbW1xc3MzqfvP83ut83J53Y3qZGVlcenSJS5evFitfwf27dtHdHQ0+fn5ODk5sXDhQiIiIoiPj5fzWwHmz5/Prl272L59+1Xr5Gf41kiiFsICDR48mP3797NhwwZzh1Lt1K9fn/j4eDIzM/nll1/o27cva9euNXdY1UJycjIvvfQScXFxJvOci9sjl77/wcvLCysrq6t6IaalpeHn52emqKqGy+fnRufOz8+P9PR0k/XFxcVcuHDBpM619nHlMa5Xpzr8Hw0ZMoQlS5awevVqk+kc/fz8KCwsJCMjw6T+P8/vrZ47FxcXHBwcqv3vgK2tLXXr1qV58+aMHz+exo0b89lnn8n5rQA7d+4kPT2dZs2aYW1tjbW1NWvXrmXq1KlYW1vj6+sr5/gWSKL+B1tbW5o3b86qVauMZXq9nlWrVhEdHW3GyCxfrVq18PPzMzl3WVlZbN261XjuoqOjycjIYOfOncY6f/31F3q9nlatWhnrrFu3jqKiImOduLg46tevj7u7u7HOlce5XKcq/x8ppRgyZAgLFy7kr7/+uuryf/PmzbGxsTH53AkJCSQlJZmc33379pl8GYqLi8PFxYWIiAhjnRudu7vtd0Cv11NQUCDntwJ06NCBffv2ER8fb1xatGhBnz59jK/lHN8Cc/dms0Tz589XdnZ2avbs2erAgQNqwIABys3NzaQX4t0qOztb7d69W+3evVsBavLkyWr37t3q5MmTSinD8Cw3Nzf122+/qb1796ru3btfc3hW06ZN1datW9WGDRtUaGioyfCsjIwM5evrq55++mm1f/9+NX/+fKXT6a4anmVtba0mTZqkDh48qN59990qPzxr4MCBytXVVa1Zs0adOXPGuOTl5RnrvPjiiyo4OFj99ddfaseOHSo6OlpFR0cb118e2tKxY0cVHx+vli9frry9va85tOW1115TBw8eVNOnT7/m0Jbq+DvwxhtvqLVr16rExES1d+9e9cYbbyiNRqP+/PNPpZSc38pwZa9vpeQc3wpJ1Ncxbdo0FRwcrGxtbVXLli3Vli1bzB2SRVi9erUCrlr69u2rlDIM0XrnnXeUr6+vsrOzUx06dFAJCQkm+zh//rx68sknlZOTk3JxcVHPPvusys7ONqmzZ88e1aZNG2VnZ6dq1KihPvroo6ti+emnn1S9evWUra2tatCggfrjjz8q7XPfCdc6r4CaNWuWsc6lS5fUoEGDlLu7u9LpdKpnz57qzJkzJvs5ceKE6tKli3JwcFBeXl7qlVdeUUVFRSZ1Vq9erZo0aaJsbW1V7dq1TY5xWXX8HXjuuedUSEiIsrW1Vd7e3qpDhw7GJK2UnN/K8M9ELee4/DRKKWWetrwQQggh/o3coxZCCCEsmCRqIYQQwoJJohZCCCEsmCRqIYQQwoJJohZCCCEsmCRqIYQQwoJJor6BgoICxowZQ0FBgblDqZbk/FYuOb+VT85x5ZLzayDjqG8gKysLV1dXMjMzcXFxMXc41Y6c38ol57fyyTmuXHJ+DaRFLYQQQlgwSdRCCCGEBav281EXFxeze/dufH190WrL970kOzsbgJSUFLKysiojvLuanN/KJee38sk5rlzV+fzq9XrS0tJo2rQp1tY3TsXV/h719u3badmypbnDEEIIIa6ybds2oqKiblin2reofX19AcPJ8Pf3N3M0QgghBJw5c4aWLVsac9SNVPtEfflyt7+/P4GBgWaORgghhChzM7dkzdqZbN26dXTr1o2AgAA0Gg2LFi0yWa+UYvTo0fj7++Pg4EBsbCxHjhwxT7BCCCGEGZg1Uefm5tK4cWOmT59+zfUTJ05k6tSpfPnll2zduhVHR0c6depEfn7+HY5UCCGEMA+zXvru0qULXbp0ueY6pRRTpkxh1KhRdO/eHYDvv/8eX19fFi1axBNPPHEnQxVCCCHMwmLvUScmJpKamkpsbKyxzNXVlVatWrF58+brJuqCggKTx81d7t4vhBA3o6SkhKKiInOHIao4GxsbrKysKmRfFpuoU1NTAa7qEefr62tcdy3jx49n7NixlRqbEKL6UUqRmppKRkaGuUMR1YSbmxt+fn5oNJrb2o/FJupb9eabbzJixAjj+5SUFCIiIipm5yXF8Nc4qH0f1GlfMfsUQliEy0nax8cHnU53239cxd1LKUVeXh7p6ekAtz002GITtZ+fHwBpaWkmHzItLY0mTZpcdzs7Ozvs7OyM7yvyaTbnVk3Ba9NnsOt/8J914BZUYfsWQphPSUmJMUl7enqaOxxRDTg4OACQnp6Oj4/PbV0Gt9hnfdeqVQs/Pz9WrVplLMvKymLr1q1ER0ff8XjOZF6iw/p67NXXgksX4KdnoPjunnpNiOri8j1pnU5n5khEdXL55+l2+zyYNVHn5OQQHx9PfHw8YOhAFh8fT1JSEhqNhuHDh/P++++zePFi9u3bxzPPPENAQAA9evS447H6uzrwULNaDCoaTiZOcHoXLBt5x+MQQlQeudwtKlJF/TyZNVHv2LGDpk2b0rRpUwBGjBhB06ZNGT16NACvv/46Q4cOZcCAAURFRZGTk8Py5cuxt7c3S7yjukZg61WTYYWD0aOBnbNg9xyzxCKEEOLuYNZEfd9996GUumqZPXs2YPg2Mm7cOFJTU8nPz2flypXUq1fPbPE62Frx2eNN2UgTphT1MhT+MQLO7DVbTEIIUdFq1qzJlClTbrr+mjVr0Gg0ld5jfvbs2bi5uVXqMSyRxd6jtlSRga68/EA9ppX0YK1qCsX58OP/waWL5g5NCHGX0Wg0N1zGjBlzS/vdvn07AwYMuOn6rVu35syZM7i6ut7S8cSNSaK+BS+2q0NUTS+GFQwkVesHGSdhwX9Arzd3aEKIu8iZM2eMy5QpU3BxcTEpe/XVV411lVIUFxff1H69vb3L1bHO1ta2QsYLi2uTRH0LrLQaJj/eGL2dG/0vDaNYawdHVsD6SeYOTQhxF/Hz8zMurq6uaDQa4/tDhw7h7OzMsmXLaN68OXZ2dmzYsIFjx47RvXt3fH19cXJyIioqipUrV5rs95+XvjUaDd988w09e/ZEp9MRGhrK4sWLjev/een78iXqFStWEB4ejpOTE507d+bMmTPGbYqLixk2bBhubm54enoycuRI+vbtW+7OwjNmzKBOnTrY2tpSv359/ve//xnXKaUYM2YMwcHB2NnZERAQwLBhw4zrv/jiC0JDQ7G3t8fX15dHH320XMe+UyRR36JAdx3v9WjI36ombxX2MxSu/hCOrrzhdkKIqkEpRV5hsVkWpVSFfY433niDjz76iIMHD9KoUSNycnJ48MEHWbVqFbt376Zz585069aNpKSkG+5n7Nix9O7dm7179/Lggw/Sp08fLly4cN36eXl5TJo0if/973+sW7eOpKQkkxb+hAkTmDNnDrNmzWLjxo1kZWVdNYPiv1m4cCEvvfQSr7zyCvv37+c///kPzz77LKtXrwbg119/5dNPP+Wrr77iyJEjLFq0iMjISMDQmXnYsGGMGzeOhIQEli9fzr333luu498pFvvAk6qgR9Ma/HUonZ/2tKOt/XG6FcfB4pdg2G6wtjV3eEKI23CpqISI0SvMcuwD4zqhs62YP8/jxo3jgQceML738PCgcePGxvfvvfceCxcuZPHixQwZMuS6++nXrx9PPvkkAB9++CFTp05l27ZtdO7c+Zr1i4qK+PLLL6lTpw4AQ4YMYdy4ccb106ZN480336Rnz54AfP755yxdurRcn23SpEn069ePQYMGAYaRQ1u2bGHSpEncf//9JCUl4efnR2xsLDY2NgQHB9OyZUsAkpKScHR05KGHHsLZ2ZmQkBDjCCRLIy3q2/Rej4YEuNrzak4f9ri2h6d+lCQthLAYLVq0MHmfk5PDq6++Snh4OG5ubjg5OXHw4MF/bVE3atTI+NrR0REXFxfjIzKvRafTGZM0GB6jebl+ZmYmaWlpxqQJYGVlRfPmzcv12Q4ePEhMTIxJWUxMDAcPHgTgscce49KlS9SuXZsXXniBhQsXGu/TP/DAA4SEhFC7dm2efvpp5syZQ15eXrmOf6dIi/o2uTrY8EnvJjz1zRa6pz3Pl+e86Oxn7qiEELfLwcaKA+M6me3YFcXR0dHk/auvvkpcXByTJk2ibt26ODg48Oijj1JYWHjD/djY2Ji812g06G/QgfZa9Svykv7NCAoKIiEhgZUrVxIXF8egQYP4+OOPWbt2Lc7OzuzatYs1a9bw559/Mnr0aMaMGcP27dstbgiYtKgrQHQdTwbcWxuANxfsJT0rH5K3wf5fzRyZEOJWaTQadLbWZlkqs/f0xo0b6devHz179iQyMhI/Pz9OnDhRace7FldXV3x9fdm+fbuxrKSkhF27dpVrP+Hh4WzcuNGkbOPGjSYTMTk4ONCtWzemTp3KmjVr2Lx5M/v27QPA2tqa2NhYJk6cyN69ezlx4gR//fXXbXyyyiEt6gryygP12XDkHH+fzmL6nJ8Zc/ZlNFor8KoHfpHmDk8IIQAIDQ1lwYIFdOvWDY1GwzvvvHPDlnFlGTp0KOPHj6du3bqEhYUxbdo0Ll68WK4vKa+99hq9e/emadOmxMbG8vvvv7NgwQJjL/bZs2dTUlJCq1at0Ol0/PDDDzg4OBASEsKSJUs4fvw49957L+7u7ixduhS9Xk/9+vUr6yPfMmlRVxBbay2fPdEEO2st359045THPVCvM7jXNHdoQghhNHnyZNzd3WndujXdunWjU6dONGvW7I7HMXLkSJ588kmeeeYZoqOjcXJyolOnTuV6RHSPHj347LPPmDRpEg0aNOCrr75i1qxZ3HfffYBhPuivv/6amJgYGjVqxMqVK/n999/x9PTEzc2NBQsW0L59e8LDw/nyyy+ZN28eDRo0qKRPfOs06k7fNLjDTp06RVBQEMnJyQQGBlb68b7ffILRv/2Ns3Uxvw65n3p+LpV+TCHE7cnPzycxMZFatWqZbS6Bu51eryc8PJzevXvz3nvvmTucCnGjn6vy5CZpUVewp+8J4f763mQXWzNsfjwFxSWgFCRv//eNhRDiLnHy5Em+/vprDh8+zL59+xg4cCCJiYk89dRT5g7N4kiirmAajYaJjzbG09GWQ6nZTF5xAH7uC98+IA9DEUKIUlqtltmzZxMVFUVMTAz79u1j5cqVhIeHmzs0iyOJuhJ4O9sxoZdhzOHMDUmkFukABb8+DxdPmjc4IYSwAEFBQWzcuJHMzEyysrLYtGmTxT4ZzNwkUVeS2AhfnmoVjFLw2IkeFPs1Ncyw9dMzUJRv7vCEEEJUEZKoK9GoruHU9nIkObuEMfYjUQ4ecCYelr1u7tCEEEJUEZKoK5HO1popTzTBWqvhh0N6NjT6CNDArv/Crv/96/ZCCCGEJOpK1ijQjZcfqAfAwC1uZESXtqb/eAVOx5svMCGEEFWCJOo74MV2dYiq6U5OQTEvHLsXfWhnKCmAn56GvOtPEyeEEEJIor4DrLQaJvdugrOdNduTMvnaa6ThiWUZSbBgAJjh8X1CCCGqBknUd0iQh45xPQyPppu4NpWEdl+AtT0cjYN1E80cnRDibnbfffcxfPhw4/uaNWsyZcqUG26j0WhYtGjRbR+7ovZzI2PGjKFJkyaVeozKJIn6DurRpAYPNfKnRK94cWURBZ0/MaxY8xEciTNvcEKIKqdbt2507tz5muvWr1+PRqNh79695d7v9u3bGTBgwO2GZ+J6yfLMmTN06dKlQo9V3UiivoM0Gg0f9IgkwNWexHO5jElqDC2eAycfsHX89x0IIcQV+vfvT1xcHKdOnbpq3axZs2jRogWNGjUq9369vb3R6XQVEeK/8vPzw87O7o4cq6qSRH2HuepsmNS7MRoNzNuWRFzwy/Cf9RDS2tyhCSGqmIceeghvb29mz55tUp6Tk8PPP/9M//79OX/+PE8++SQ1atRAp9MRGRnJvHnzbrjff176PnLkCPfeey/29vZEREQQF3f1FcCRI0dSr149dDodtWvX5p133qGoqAgwTDc5duxY9uzZg0ajQaPRGGP+56Xvffv20b59exwcHPD09GTAgAHk5OQY1/fr148ePXowadIk/P398fT0ZPDgwcZj3Qy9Xs+4ceMIDAzEzs6OJk2asHz5cuP6wsJChgwZgr+/P/b29oSEhDB+/HgAlFKMGTOG4OBg7OzsCAgIYNiwYTd97Fsh81GbQes6XgxoW5uv1h1n5G8JNB7eFp/LK0/tAM+64OBmxgiFEEaFueXfxsoOrEr/vJYUG0Z5aLRg4/Dv+y3H1TVra2ueeeYZZs+ezdtvv22cy/nnn3+mpKSEJ598kpycHJo3b87IkSNxcXHhjz/+4Omnn6ZOnTq0bNnyX4+h1+t55JFH8PX1ZevWrWRmZprcz77M2dmZ2bNnExAQwL59+3jhhRdwdnbm9ddf5/HHH2f//v0sX77cOFe0q6vrVfvIzc2lU6dOREdHs337dtLT03n++ecZMmSIyZeR1atX4+/vz+rVqzl69CiPP/44TZo04YUXXrip8/bZZ5/xySef8NVXX9G0aVO+++47Hn74Yf7++29CQ0OZOnUqixcv5qeffiI4OJjk5GSSk5MB+PXXX/n000+ZP38+DRo0IDU1lT179tzUcW+VRSfqkpISxowZww8//EBqaioBAQH069ePUaNGlWtycUs0omM91h85x4EzWbz2815mPxuFJnEtzH0C/CLh6YVg52TuMIUQHwaUf5vHZkODnobXh36Hn/tBSBt49o+yOlMiIe/81duOySzXoZ577jk+/vhj1q5da5yHedasWfTq1QtXV1dcXV159dVXjfWHDh3KihUr+Omnn24qUa9cuZJDhw6xYsUKAgIM5+LDDz+86r7yqFGjjK9r1qzJq6++yvz583n99ddxcHDAyckJa2tr/Pz8rnusuXPnkp+fz/fff4+jo+ELy+eff063bt2YMGECvr6+ALi7u/P5559jZWVFWFgYXbt2ZdWqVTedqCdNmsTIkSN54oknAJgwYQKrV69mypQpTJ8+naSkJEJDQ2nTpg0ajYaQkBDjtklJSfj5+REbG4uNjQ3BwcE3dR5vh0Vf+p4wYQIzZszg888/5+DBg0yYMIGJEycybdo0c4d22+ysrfjsiSbYWWtZe/gs328+CTpPsLYztKa1VuYOUQhRBYSFhdG6dWu+++47AI4ePcr69evp378/YGjwvPfee0RGRuLh4YGTkxMrVqwgKSnppvZ/8OBBgoKCjEkaIDo6+qp6P/74IzExMfj5+eHk5MSoUaNu+hhXHqtx48bGJA0QExODXq8nISHBWNagQQOsrMr+Rvr7+5Oenn5Tx8jKyuL06dPExMSYlMfExHDw4EHAcHk9Pj6e+vXrM2zYMP78809jvccee4xLly5Ru3ZtXnjhBRYuXEhxcXG5Pmd5WXSLetOmTXTv3p2uXbsChm9p8+bNY9u2bWaOrGKE+jrz1oPhvLv4bz5cepDWQ9sQ2j/OMMba2tbc4QkhAN46Xf5trK7oHBXWzbAPzT/aRcP33V5cV+jfvz9Dhw5l+vTpzJo1izp16tCuXTsAPv74Yz777DOmTJlCZGQkjo6ODB8+nMLCwgo7/ubNm+nTpw9jx46lU6dOuLq6Mn/+fD755JMKO8aVbGxsTN5rNBr0Ffg8imbNmpGYmMiyZctYuXIlvXv3JjY2ll9++YWgoCASEhJYuXIlcXFxDBo0yHhF459xVRSLblG3bt2aVatWcfjwYQD27NnDhg0bqlVX/meiQ2hXz5uCYj2D5uwiy7lWWZJWCuLnQcnNd5IQQlQwW8fyL1ZXtIGsrA1lV96fvtF+b0Hv3r3RarXMnTuX77//nueee854e3Djxo10796d//u//6Nx48bUrl3b+Df1ZoSHh5OcnMyZM2eMZVu2bDGps2nTJkJCQnj77bdp0aIFoaGhnDxpOqWvra0tJSUl/3qsPXv2kJtbdv9+48aNaLVa6tevf9Mx34iLiwsBAQFs3LjRpHzjxo1ERESY1Hv88cf5+uuv+fHHH/n111+5cMHwJEkHBwe6devG1KlTWbNmDZs3b2bfvor74vVPFt2ifuONN8jKyiIsLAwrKytKSkr44IMP6NOnz3W3KSgooKCgwPg+Ozv7ToR6yzQaDR8/1ohu0zZwJD2HwXN28V2/KGystBD3DmyaBkf+hF7fyOVwIcQ1OTk58fjjj/Pmm2+SlZVFv379jOtCQ0P55Zdf2LRpE+7u7kyePJm0tDSTpHQjsbGx1KtXj759+/Lxxx+TlZXF22+/bVInNDSUpKQk5s+fT1RUFH/88QcLFy40qVOzZk0SExOJj48nMDAQZ2fnq4Zl9enTh3fffZe+ffsyZswYzp49y9ChQ3n66aeN96crwmuvvca7775LnTp1aNKkCbNmzSI+Pp45c+YAMHnyZPz9/WnatClarZaff/4ZPz8/3NzcmD17NiUlJbRq1QqdTscPP/yAg4ODyX3simbRLeqffvqJOXPmMHfuXHbt2sV///tfJk2axH//+9/rbjN+/HhjBwpXV9eb/mE0Jx9ne77tG4XO1or1R87xzqL9KKWg5r2gtYG/F8DiofKoUSHEdfXv35+LFy/SqVMnk/vJo0aNolmzZnTq1In77rsPPz8/evTocdP71Wq1LFy4kEuXLtGyZUuef/55PvjgA5M6Dz/8MC+//DJDhgyhSZMmbNq0iXfeecekTq9evejcuTP3338/3t7e1xwiptPpWLFiBRcuXCAqKopHH32UDh068Pnnn5fvZPyLYcOGMWLECF555RUiIyNZvnw5ixcvJjQ0FDD0YJ84cSItWrQgKiqKEydOsHTpUrRaLW5ubnz99dfExMTQqFEjVq5cye+//46np2eFxngljVJKVdreb1NQUBBvvPEGgwcPNpa9//77/PDDDxw6dOia2/yzRZ2SkkJERATJyckEBgZWesy3Y9XBNF74fgd6BW90CePFdnXgwG/w87OgSqBFf+j6CVTxHu9CWJr8/HwSExOpVasW9vb25g5HVBM3+rk6deoUQUFBN5WbLLpFnZeXh1ZrGqKVldUNOw3Y2dnh4uJiXJydnSs7zArTIdyX0Q8ZrgB8tOwQS/edgYju0PNLQAM7voU/RxnuXQshhLgrWPQ96m7duvHBBx8QHBxMgwYN2L17N5MnT+a5554zd2iVpl9MLU6cz2P2phO8/GM8/q72NG3UG4rzDZe/N38ONjpo//a/70wIIUSVZ9Et6mnTpvHoo48yaNAgwsPDefXVV/nPf/7De++9Z+7QKtU7D0XQIcyHgmI9L3y/g+QLedDsGehSOsvWuomwvnKGPQghhLAsFp2onZ2dmTJlCidPnuTSpUscO3aM999/H1vb6j3G2EqrYeqTTYnwd+FcTiHPzd5O5qUiaPUfiB1rqLRqHGyZYd5AhRBCVDqLTtR3M0c7a77rF4Wfi71x2FZRiR7aDId2bxgqLX8Ddswya5xCCCEqlyRqC+bnas+3/Vqgs7Viw9FzjFpYOmzrvjegdelsLUtehj3zzRuoENVERT7dSoiK+nmy6M5kAhoEuDLtyaa88P0OftyRTE0vRwbeVwceGAdFl2DnLLCW4SRC3A5bW1u0Wi2nT5/G29sbW1vbKj/xjzAfpRSFhYWcPXsWrVZ727drJVFXAR3CfXm3WwPeXfw3E5YfIsRTx4OR/obOZc2eBv/G5g5RiCpNq9VSq1Ytzpw5w+nTt/BsbyGuQafTERwcfNUw4/KSRF1F9G1dk8RzucZhW36u9jQLdjdN0hnJcPEE1GprtjiFqKpsbW0JDg6muLj4X59JLcS/sbKywtraukKuzEiirkLeeSiCUxfzWHkwnQHf72DhoBiCPHSGlZkpMOtByD1rmMs65Opp6IQQN6bRaLCxsam0WZCEuBXSmawKsdJq+OyJpjQIMAzbevbysC0AR2/wrg8uAeAWZN5AhRBCVBhJ1FWMo5013/Y1DNs6mp7DoDk7DcO2rG3h8f/Bc8vB1bKfaS6EEOLmSaKugq4ctrXx6PmyYVs2DuDkU1bxwGJIv/bkJUIIIaoGSdRVVIMAVz5/qilaDfy4I5kZa4+ZVji0FH7uC98/DOePXXsnQgghLJ4k6iqsfZgvYx5uAMDE5Qn8sfdM2crge8A7HHLS4L8Pw75foDDPTJEKIYS4VZKoq7hnomvybExNAF7+KZ5dSRcNK3Qe8Mwi8AyFrFPwa3+YFAoLB8LxNaCX4SdCCFEVSKKuBkZ1jSA23IfCYj0v/Ld0ti0w3K9+bgXc+xq4hUBhDuyZC993h08bwp/vQNrf5g1eCCHEDUmirgauHLZ1PreQfrO2kZl3ediWJ7QfBS/tMSTt5s+CvRtkn4ZNU2FGa5gRAxs/g4Jss34OIYQQV5NEXU1cnm3L39WeY2dzGThnJ4XFVzwQXqMx3LfuNgVePQyP/wBhD4HWBtL2w9qJoLEqqy+TEwghhEWQRF2N+LrY823fKBxtrdh07DyjFu0zDNv6J2s7CO8GT8wxJO2HPoV7XwXb0qecKQXftIdf+kPmqTv7IYQQQpiQRF3NRAS48PlTzdBq4Kcdp/hizb8MzdJ5QIvnoM3LZWVpf8Pp3XBoCdg5l5XnXTAkcSGEEHeMJOpq6P4wH8aWDtv6eEUCS/aWczYg3wbwwl/Q9ROwdy0r/19P+DwK1n0MF09WYMRCCCGuRyblqKaejq5J4rk8vtuYyIif9nDyfB7/d08Irg43MdmARgM1mhuWy7LT4GwCFF+Cv943LMGtoW4HcPYzPGv8ysVG5sgWQoiKoFHXvIlZfZw6dYqgoCCSk5MJDLy7noFdolcM/GEnfx5IA8DJzpqnWgXzXEwt/FxvIZHmZ8HB32HvfEhcD9zgR8fOxZCwn14A7jUNZSc2QPpBCGwBAU0NZZd//CpgKjghhKgqypObpEVdjVlpNXzRpxm/xZ/mq3XHOJyWw8x1x5m1MZGeTWsw4N461PVxuvkd2rtA0z6GJTMF9v9qSLy5ZyE3HXLPQU466IugIMuw2F6x/wO/wbaZ0PbVskR9MRGmtzJtjTv5gKMXOPqUvvc2vHbyAZ0naK2uHZ8QQlRDkqirOWsrLb2aB9KzaQ1WJ6Tz5dpjbD9xkZ92nOLnnad4INyX/7SrQ/MQ9/Lt2LUGxAy7ulwpyM8sTd5nwcGjbJ1vQ0Nvc7/IsrLcc1BSCFkphuVfaQzJ+vk48KhtKDq6Cs7sgZAYCG5lKNPrQV9smFVMCCGqMLn0fRfaefICX649TlzpJXGAljU9ePG+2txf3wfNnbwMXVIE2ammLfLLSf6fr/POY7zc/nqiocc6wB+vwPZvDE9gaz/KUHb+GExrZugMd7k1bmytexuSvc7T0HLXeYLOCxzcwUq+uwoh/qEw1/B3CA24h1TILuXSt7ih5iEefP2MB0fTs/lq7XEWxaew7cQFts2+QH1fZ/7TrjbdGgdgY3UHBgVY2YBbkGH5NyXFhmSde9bwdLXLAlsaJhy5svNbTrrh3/xMw3L+yE0Eo4GhO8GzjuHtvl8gcS3U6wxhXQ1lxQWGB8ToShO8rWPl319XyrBoZZCGEBUmJx0ykw0NhNxzhr8reecg9/zVr4svGbap1wWemn/HQ5UWteBM5iVmbTzBnC0nyS00TNZRw82B/m1q8XhUEI52VfD7nF4P+RmlrfL0a7TOLxiSft45w7+XSiczubKlvuRl2PEdtBsJ979lKDt7GKZHlR3H2r40aXsYWudWtoYJT/TFpUsJqBLo/b2hdzzAxqmw63to+n/QZrihLCMZZt5n2Ebpr94eDFcGXAMNtx1cgwyvGzwCLv6VfDLFXafoUunvx3nDF2Qnn7Iv0yVFcP6o4UmG3vXKtsk5C8X5oNEaFq1V2WuNxlDf+P6KcqvSkSh6veH3U18Mzv5lX0yzTht+P42/V6W/E5dfX1VWun3wPWXxzn/KkIz7Li57NsTioYbfw5tlZWcY5fLkvNs7t6WkRS3Kxd/VgbceDGfw/XX5YctJZm08QUrGJcYtOcDUv47wzD0h9G1dE08nO3OHevO0WkPy1HkAYf9ev6TY8MfA4Yp79WEPGX7hQ2LKyoovgUuN0nvrBYY/TFmnDMuNFF0xxWjeeUML/3KrHwx/sPLO3XgfuaVfOk7vKisLaV2WqLd+BZunQ5M+cN9IQ5m+BI6tLk3ugaYPsBF3h+ICw9MILzsSBxcS4dKFsmRs/OJa+u/lFuRl9wyGzh8aXuekwRf3GBLXO1f8DC8eCoeXlS+2xk9Czy9L48yHT0oT/5spYFfaEXXVe4bJhMojtBP0+cnw2srGMOKkKM/wReDy74BLDcPi6GX4su3oVXZbzPj6crmXoWOsmUanWHyiTklJYeTIkSxbtoy8vDzq1q3LrFmzaNGihblDq3ZcHWwYfH9d+repxa+7TjFz3XFOns9j6l9Hmbn+OL1bBPFC29oEeejMHWrFs7I29C6/Ut0OhuVK/o1hxAHDpejC3LIWeW5p61xfbGg5aK0NLQpt6WtHn7J9NO8HoR3BJaCszMkXBm0p3fbK7a0Ni9Ib7uVnnjIsWaX/ul1xv+xCImSchKLcsrLsMzCnV9l7O9fSVnlgWfJ28ChtjehL/y0xtPYd3AzbHFsNJzdBUEsIfcBQdukirB5fVv+f21/+V2tleJ68VekSPaTs1sKpnXD8L/CJKLu1oBTsmWe4MqG1Nvx7eVutTel7a8N5Kikq/bJUYOig6Ohl2Me5o3BiHTgHQP3OZZ995VjDH+viAkMHxuKC0u0LDf+WFBnKwJDYrGwN8YbGGsrOJhj6QrgFQ+uhZfvd86Nhv9b2hs6L1val29uZllnZll4tKSlNDqVXbgqyDU8D1NpA4BW3bxLXlbYki/9xleYa74vyDYm3Tnuo38Wwfep++CbW8P/4yqGy/a6bBMlb+Fdaa8PPhrW96RfYyx06rf7RUdPK2lD38mdUem44hBNMp9vVWhv2rbUqu4oEpf1MvA3rr/X7ca2yyx1NL3t4GtjoDPu57L43DEsVYNGJ+uLFi8TExHD//fezbNkyvL29OXLkCO7u5eyhLMrF3saKPq1CeCIqmOX7U/ly7TH2pWTy/eaTzNmaRNdIf/7TrjYNAlz/fWfVlUZj+MZv51Q2TvxmedQyLFeysgGf8Btv5+QD/o2uv77tCIjobvrHqDAPfCMN9+LyM6AgE9IzIf1fpjet17ksUSeugw2TodXAskRddAm2fXXjfVxLoyfKEnXyFsODcyIfK0vUJUWwaGD59/vk/LIEdWqb4bZFnQ6miXrrV6ZfYm5G5GNlry+eMAwv9G9imqj/eh8yk8q3347vl+0j/RB818nwc/TSnrI6K96G1L3l26+NQ9l5sHcxtIzz9IYvQJdbgzXblA111HmWXnkqfe3gUfbezvnaLUjXGvD68avLH//h6rLL/StUaeL+53Jlsre2hTEZV++jy0eG5XZEPnp725uZRSfqCRMmEBQUxKxZs4xltWrVusEWoiJZaTV0beTPg5F+bD52nhlrj7H+yDkW7znN4j2nubeeNwPa1qZ1HU+0Wnlgidk5lfZuv5J3PRi4wfC6IMcwBC4zubRlnmL4Nz/TcKvgcstEY2XoJHdZYBREvVB2zw8Mf8TbvmK6jfYfrzWlLaOSIkMLtqTI0IK/zCccmj1j2glQ6aFubOk2RYYx+SWFhlsTJYWl74sMLbHLrW1rO0OCuswtxHDbwu8fX2qiBxvisbIzJAWTf+1KW+6ll4gvt7SDruiP4F7LMLLAydd0v3U7GG5jFOdf0UrPN2xvUlZwxXm64gmBtjrwqGNIgFfyb2S43HrllRnj8o/3VraG5BrSumx75wBD4r9yiCRAh3e4YzSa0mQvHSFvh0V3JouIiKBTp06cOnWKtWvXUqNGDQYNGsQLL7xw0/uQzmQVa39KJl+tO84fe0+jL/3JCfHU8XhUEI82D8THWR4dKoQQ/6Y8ucmiE7W9veGP/ogRI3jsscfYvn07L730El9++SV9+/a95jYFBQUUFBQY36ekpBARESGJuoIlnc/j2w3HWbArheyCYgCstRpiw315slUwbet6SStbCCGuo9okaltbW1q0aMGmTZuMZcOGDWP79u1s3rz5mtuMGTOGsWPHXlUuibpy5BUWs2TvGeZvS2JXUoaxvIabA09EBfFYi6Bbe664EEJUY+VJ1Ld04yA5OZlTp8qGo2zbto3hw4czc+bMW9nddfn7+xMREWFSFh4eTlLS9TtuvPnmm2RmZhqXAwcOVGhMwpTO1preLYJYMCiGFcPvpV/rmrjYW5OScYlP4g7T+qNVPP/f7aw6mEZxid7c4QohRJVzS4n6qaeeYvXq1QCkpqbywAMPsG3bNt5++23GjRtXYcHFxMSQkJBgUnb48GFCQq7/CDc7OztcXFyMi7OzjBu9U+r7OTPm4QZsezuWTx9vTMuaHugVrDyYTv//7qDNhNVMjjtMSsalf9+ZEEII4BYT9f79+2nZsiUAP/30Ew0bNmTTpk3MmTOH2bNnV1hwL7/8Mlu2bOHDDz/k6NGjzJ07l5kzZzJ48OAKO4aoePY2VvRsGshPL0azckQ7XmhbC3edDalZ+UxddYQ2E/6i36xtLN+fSpG0soUQ4oZuaXhWUVERdnaGYQwrV67k4YcfBiAsLIwzZ85UWHBRUVEsXLiQN998k3HjxlGrVi2mTJlCnz59KuwYonLV9XHi7a4RvNqpPn/+nca8bUlsOnaeNQlnWZNwFm9nOx5rHsgTUcEEe1bDB6kIIcRtuqXOZK1ateL++++na9eudOzYkS1bttC4cWO2bNnCo48+anL/2txkeJblOXEul/nbk/ll5ynO5ZT10G9T14snWgbRMcIPW2sZdymEqL4qvdf3mjVr6NmzJ1lZWfTt25fvvvsOgLfeeotDhw6xYMGCW4u8EkiitlxFJXpWHUxj7rZk1h85y+WfRE9HW3o1D6RpkBu+rvb4udjj7Wx3Z2bzEkKIO+CODM8qKSkhKyvL5HGeJ06cQKfT4ePjc4Mt7yxJ1FVD8oU8ftqRzE87kknLKrhqvUYDXk52+LnY4+tij5/rla8NydzX1R5nO+s7O5+2EELcgkqfPevSpUsopYxJ+uTJkyxcuJDw8HA6dep0K7sUd7kgDx2vdKzPSx1CWZNwliV7T5N88RKpmfmkZeVTrFeczS7gbHYB+1Iyr7sfna2VSQL3dbHHz8Wu7LWrPd5OdlhL61wIUUXcUqLu3r07jzzyCC+++CIZGRm0atUKGxsbzp07x+TJkxk48BYeqi8EYG2lJTbCl9iIsucp6/WK87mFpGXlk5qZT2pW/jVfZ+UXk1dYwvFzuRw/d/3JF3S2VnRu4McjzQKJruOJlTxBTQhhwW4pUe/atYtPP/0UgF9++QVfX192797Nr7/+yujRoyVRiwql1WrwdrbD29mOhjWuP2NXXmExaVkFxlb4mcyrE3p6dgF5hSUs2J3Cgt0p+LnY06NpDXo1q0Gor4y5F0JYnltK1Hl5ecYHifz555888sgjaLVa7rnnHk6ePFmhAQpxs3S21tTysqaWl+N165ToFfHJGSzYdYrf95wmNSufL9ce48u1x2gU6MojTWvQrXEAnk52dzByIYS4vlu6UVe3bl0WLVpEcnIyK1asoGPHjgCkp6fj4uJSoQEKUZGstBqah7jzQc9Ito+KZUafZsSG+2Kt1bD3VCZjfj9Aqw9X8fx/d7Bs3xkKikv+fadCCFGJbqlFPXr0aJ566ilefvll2rdvT3R0NGBoXTdt2rRCAxSisthZW9El0p8ukf6czyng9z2nWbA7hb2nMll5MI2VB9NwdbDhoUb+PNIskGbBbtKjXAhxx93y8KzU1FTOnDlD48aN0WoNDfNt27bh4uJCWFhYhQZ5O2R4liivI2nZLNidwsJdKaRm5RvLa3rqeKRZID2b1iDIQ56iJoS4dXd0msvLTyGz1CQoiVrcqhK9Ysvx8/y66xTL96eSV1h2GbxlLQ96NatBl0h/XOxtzBilEKIqqvRpLvV6PePGjcPV1ZWQkBBCQkJwc3PjvffeQ6+XSRZE9WCl1RBT14vJvZuw/e1YPnmsMTF1PdFoYFviBUb+uo+o91cydN5uViekyzSeQohKcUv3qN9++22+/fZbPvroI2JiYgDYsGEDY8aMIT8/nw8++KBCgxTC3BztrOnVPJBezQM5nXGJRfEp/LrzFMfO5vL7ntP8vuc0Xk52xIb70CjQjUaBrtTzdZZnlgshbtstXfoOCAjgyy+/NM6addlvv/3GoEGDSElJqbAAb5dc+haVRSnFvpRMFuxK4bf4FC7mFZmst7XSEu7vTGSgK41quNEoyJW63k7yVDQhROU/QvTChQvX7DAWFhbGhQsXbmWXQlQ5Go2mtPXsxlsPhrPh6Fl2nLjI3lOZ7D2VQVZ+MXtOZbLnVCaQBIC9jZYGAa5E1nClUaBhqe3lhFaejiaEuI5bStSNGzfm888/Z+rUqSbln3/+OY0aNaqQwISoSmyttbQP86V9mOHRp0opki7ksfdUJvtSDIl7f0oWOQXF7Dx5kZ0nLxq3dbS1omFp4o4MdKNRDVdCPHUyFEwIAdxiop44cSJdu3Zl5cqVxjHUmzdvJjk5maVLl1ZogEJURRqNhhBPR0I8HenWOAAwPLP8+Llc9qVkGBL4qUz2n84kt7CErYkX2JpYdjXKxd7acMm8NHE3CnKjhpuDuT6OEMKMbnl41unTp5k+fTqHDh0CIDw8nAEDBvD+++8zc+bMCg3ydsg9amHJikv0HDuby55TGew7lcnelEwOns6i8Bo9yO+r783rncKICJCn/wlR1d3RcdRX2rNnD82aNaOkxHIeuyiJWlQ1hcV6Dqdll14yz2RfSgYHz2RToldoNNCjSQ1GPFBPHroiRBVW6Z3JhBCVx9ZaS8MarjSs4cqTLQ1liedymfRnAn/sPcPC3Sks2XuaPq1CGNq+rkwgIkQ1J+NEhKgCank5Mv2pZvw+pA1t6npRVKKYvekE7T5ew2crj5BbUGzuEIUQlUQStRBVSGSgKz8834of+rcisoYrOQXFfLryMO0+Xs1/N52gsFiejiZEdVOuS9+PPPLIDddnZGTcTixCiJvUJtSL1nVi+GPfGT75M4ET5/N4d/HffLshkVc61qNbowAZmy1ENVGuRO3q6vqv65955pnbCkgIcXO0Wg3dGgfQuaEf87cn89nKIyRdyOOl+fHMXHec1zuHcW+ol4zHFqKKq9Be35ZIen2Lu0VeYTHfbUjky7XHySm9Z926jicjO4fROMjNvMEJIUxU+uxZQgjLo7O1Zkj7UNa9fj/929TC1krLpmPn6T59I4Pm7OT42RxzhyiEuAWSqIWoZjwcbXnnoQj+erUdvZoFotHA0n2pPPDpOt5csI+0rHxzhyiEKIcqlag/+ugjNBoNw4cPN3coQli8QHcdn/RuzLKX2tIhzIcSvWLetiTafbyaicsPkXmp6N93IoQwuyqTqLdv385XX30lk34IUU5hfi582y+Kn1+MpnmIO/lFer5Yc4x7J65m5rpj5BdZzpMEhRBXqxKJOicnhz59+vD111/j7u5u7nCEqJKianrwy4vRfP1MC0J9nMi8VMSHSw/RZsJfvLlgH6sPpUvSFsICVYlHiA4ePJiuXbsSGxvL+++/f8O6BQUFFBQUGN9nZ2dXdnhCVBkajYYHInxpH+bDgl2n+DTuMKcz85m3LYl525LQ2VrRrp63sY6bztbcIQtx17P4RD1//nx27drF9u3bb6r++PHjGTt2bCVHJUTVZqXV8FiLILo3qcHm4+eJO5DKygPppGbls2x/Ksv2p2Kl1RBV050HIvzoGOErk4AIYSYWPY46OTmZFi1aEBcXZ7w3fd9999GkSROmTJlyzW3+2aJOSUkhIiJCxlEL8S+UUuxLySTuQBpxB9I4lGp6NSrMz5kHInx5IMKXyBqu8iAVIW6D2aa5rGiLFi2iZ8+eWFlZGctKSkrQaDRotVoKCgpM1l2LPPBEiFuTdD6PuINpxB1IZfuJi5Toy/5U+LnYExvhQ8cIP+6p7YmtdZXo7iKExag2iTo7O5uTJ0+alD377LOEhYUxcuRIGjZs+K/7kEQtxO27mFvIX4fSiTuQxrojZ8krLOt05mxnTbv6hvva99X3wdXBxoyRClE1VJv5qJ2dna9Kxo6Ojnh6et5UkhZCVAx3R1t6NQ+kV/NA8otK2HTsXOkl8nTO5RSwZO8Zluw9g7VWwz21PY2XyAPcHMwduhBVnkUnaiGE5bG3saJ9mC/tw3z5oIci/lSG8b720fQcNhw9x4aj53h38d+0quXBgHtrc399H5nNS4hbZNGXviuCXPoW4s5JPJdL3IFU4g6ksePkRS7/danj7cjzbWvTs2kN7G1u3K9EiLtBtblHXREkUQthHmcyLzF74wnmbk0iu3Q2Ly8nW56Jrsn/3ROCh6OM0RZ3L0nUV5BELYR5ZecX8eP2ZGZtPEFKxiUA7G20PNo8kP5talPLy9HMEQpx50mivoIkaiEsQ3GJnqX7U5m57hj7U7IA0GjggXBfBtxbm+Yh7jI2W9w1qk2vbyFE9WFtpeXhxgF0a+TPluMX+Gb9cVYdSufPA2n8eSCNpsFuDGhbm44N/LCSjmdCGEmiFkLcURqNhug6nkTX8eRoejbfrE9kwe4UdidlMHDOLoI9dPRvU4vHWgSis5U/UULIpW8hhNmdzS7gf5tP8P2Wk2TkGebJdnWw4f/uCaZvdE18XOzNHKEQFUvuUV9BErUQVcelwhJ+2XWKb9cf58T5PABsrbT0aBrA821rU8/X2cwRClExJFFfQRK1EFVPiV4RdyCNb9YfZ8fJi8by++p7M6BtbaLreErHM1GlSWcyIUSVZqXV0LmhH50b+rHz5EW+WX+cFX+nsibhLGsSzlLP14laXo6462xx09nirrPBXWeLa+m/7job3HS2uOlssLGSCUNE1SaJWghh0ZqHuNM8pDknz+fy3YZEftpxisNpORxOy7mp7Z3trI0J3O0fifzKhG4ot8XP1V5mAxMWRRK1EKJKCPF0ZGz3hrz8QD02HTvP+dxCMnILuZhXREZeIRfzrnxdRFZ+EUpBdkEx2QXFnLp46aaO4+Vkx5tdwnikWQ25vC4sgiRqIUSV4qaz5cFI/3+tV6JXZF4qS9ym/17xOreIi3mFZF4q4kJuIedyCnjl5z3M357EuO4NCfd3uQOfSojrk0QthKiWrLQaPBxty/VM8cJiPd9uSGTqqiNsP3GRh6Zt4JnoEF5+oB4u9jLPtjAPuREjhBClbK21DLyvDqteaUfXSH9K9IpZG0/QftJaFu4+RTUfJCMslCRqIYT4hwA3B6b3acb/+rektpcj53IKePnHPTz+1RYOpWaZOzxxl5FELYQQ19E21Jtlw9vyeuf6ONhYse3EBbpO3cB7Sw6QnV9k7vDEXUIStRBC3ICdtRWD7qvLylfa0aWhHyV6xbcbEmn/yVp+i0+Ry+Gi0kmiFkKIm1DDzYEZ/9ec/z7XklpejpzNLuCl+fE8MXMLh9OyzR2eqMYkUQshRDm0q+fN8uFtea1TfexttGxNvMCDn63ngz8OkFNQbO7wRDUkiVoIIcrJztqKwffXZeWIdnRq4EuxXvH1+kQ6fLKGxXtOy+VwUaEkUQshxC0KdNfx1dMtmPVsFCGeOtKyChg2bzdPfb2VI3I5XFQQSdRCCHGb7q/vw4rh9zLigXrYWWvZfPw8XT5bz/ilB8mVy+HiNkmiFkKICmBvY8WwDqGsHNGO2HDD5fCv1h2nwydrWbJXLoeLWyeJWgghKlCQh45v+rbg274tCPbQkZqVz5C5u/m/b7ey5fh5SvSSsEX5yLO+hRCiEnQI9yWmrhdfrj3GF2uOsfHoeTYePY+Psx0PRvrzUCN/mgW7o9XKDF3ixiy6RT1+/HiioqJwdnbGx8eHHj16kJCQYO6whBDiptjbWDE8th4rX25H7xaBuNhbk55dwOxNJ3j0y83ETPiL95ccYHfSRbk0Lq5Loyz4p6Nz58488cQTREVFUVxczFtvvcX+/fs5cOAAjo6ON7WPU6dOERQURHJyMoGBgZUcsRBCXF9hsZ71R87yx94z/HkgzWTcdQ03Bx5q5M9DjQJoWMNF5sKu5sqTmyw6Uf/T2bNn8fHxYe3atdx77703tY0kaiGEJcovKmHtYUPSXnkwjbzCEuO6EE8dXSMNSTvc31mSdjVUntxUpe5RZ2ZmAuDh4WHmSIQQ4vbY21jRqYEfnRr4camwhDUJ6SzZe4ZVh9I4eT6PL9YY7m3X9nLkoUb+dG0UQH0/Z3OHLcygyrSo9Xo9Dz/8MBkZGWzYsOG69QoKCigoKDC+T0lJISIiQlrUQogqIa+wmFUH01my9zSrE85SWKw3rgv1ceKhRgF0beRPXR8nM0Ypble1vPQ9cOBAli1bxoYNG274ocaMGcPYsWOvKpdELYSoanIKill5II0le0+z7vA5CkvKknaYn7PxnnZNr5vrsyMsR7VL1EOGDOG3335j3bp11KpV64Z1pUUthKiOMi8VEXcgjT/2nmb9kXMUXzEeO8LfhXvredM21IvmIe7Y21iZMVJxM6pNolZKMXToUBYuXMiaNWsIDQ0t9z6kM5kQorrJyCtkxd+pLNl7hk3HTB+iYmetJaqmB21CvWhT14sIfxcZq22Bqk1nssGDBzN37lx+++03nJ2dSU1NBcDV1RUHBwczRyeEEObhprPl8ahgHo8K5nxOAeuOnGX9kXNsPHqOtKwCNhw9x4aj5wBw19nQuq4habep60WQh87M0YvysugW9fWGJMyaNYt+/frd1D6kRS2EuFsopTh2NseYtDcfO0/uFcO+wDD0K6auF23rehFdxxM3na2Zor27VZsWtQV/hxBCCIuj0Wio6+NMXR9nno2pRVGJnj3JGYYW9pFz7E7O4OT5PE6eT2Lu1iQ0Gois4WpsbTeT+9sWyaJb1BVBWtRCCGGQnV/EtsQLxhb3kfQck/X2NqX3t+t6ESP3tytVtWlRCyGEqDjO9jZ0CPelQ7gvAGlZ+WwoTdobjp4jPbuA9UfOsf6I4f62h6MtMXW9uK+eN/fW88bb2c6c4d+1JFELIcRdytfFnl7NA+nVPBClFEfSc4yJe8vx81zILeT3Paf5fc9pABrWcOG+ej7cV9+bJkFuWFtZ9LxO1YZc+hZCCHGVohI9u5MyWHf4LGsOp7M/JctkvYu9NW1DvWlX35v76nnj42JvpkirpmozjroiSKIWQojbl56dz/rD51hz+Czrj5wlI6/IZH24vwv3lSbtZiHu2Ehr+4YkUV9BErUQQlSsEr0iPjmDtYfPsjYhnb0pmVyZSZztrA33tusbWtz+rvLci3+SRH0FSdRCCFG5zucYOqGtSUhn3ZFzXMgtNFlf39fZkLTredOipge21tLalkR9BUnUQghx55ToFftTMlmTYLi3HZ+cYdLadrS1onVdL9rV8+ae2h7U9nK6K4eAyfAsIYQQZmGl1dA4yI3GQW68FBvKxdxC1h8tbW0fPsu5nELiDqQRdyANAFcHG5oFu9E8xJ3mIR40DnJFZyup6UpyNoQQQlQad0dbHm4cwMONA9DrFQfOZLEmIZ31R86x51QGmZeKWJ1wltUJZwFDoo/wd6F5iDvNQtxpHuJODbe7+x63XPoWQghhFkUleg6eyWLnyYvsOHmRnScukpqVf1U9f1d7Q9IONiTuiACXKt+rXO5RX0EStRBCVB2nMy6x8+RF43LgTJbJNJ5geNRpo0A3WpS2uJsFu+PuWLUmF5F71EIIIaqkADcHAtwc6NY4AIC8wmL2JGeyK6kseWdeMjyzfFviBeN2tb0djS3uZiHu1PF2wqqadFKTRC2EEMJi6Wytia7jSXQdTwD0esXxczkmre5jZ3M5Xrr8vPMUYGh1R/i70LCGKw0DXGlQw4VQH+cqOTRMErUQQogqQ6stm8rz8ahgAC7mFrI7+SI7Thjude9PySSvsIRdSRnsSsowbmtrpaW+nzMNa7jQIMCVhjVcCfNztvipPSVRCyGEqNLcHW1pH+ZL+zDDrGAlesWJ87nsT8nk79NZ7E/JZH9KJln5xexLyWRfSiaQDBh6mYf6OJUmbkMLPNzfBSc7y0mPlhOJEEIIUQGstBrqeDtRx9uJ7k1qAKCU4tTFS4akfTqT/SmGBH4+t5BDqdkcSs3m112G7TUaqOXlSMPLyTvAlQYBrrjqbMzyeSRRCyGEqPY0Gg1BHjqCPHR0ifQHDMk7LavAJHn/fTqTM5n5xnvei0un+AQI8nCgRYgHnz7e5I7GLolaCCHEXUmj0eDnao+fqz2xEb7G8nM5BcZL5n+XJvCkC3kkX7iEp2PuHY9TErUQQghxBS8nO9rVM0wicllmXhF/n8lEr7/z8UiiFkIIIf6Fq86G1nW8zHLsqjegTAghhLiLSKIWQgghLJgkaiGEEMKCSaIWQgghLJgkaiGEEMKCVfte3/rSvvRnzpwxcyRCCCGEweWcpL+J8V7VPlGnpaUB0LJlSzNHIoQQQphKS0sjODj4hnU0Sil1wxpVXHFxMbt378bX1xet9vau9GdnZxMREcGBAwdwdnauoAirNzln5SfnrPzknJWfnLPyq8hzptfrSUtLo2nTplhb37jNXO0TdUXKysrC1dWVzMxMXFxczB1OlSDnrPzknJWfnLPyk3NWfuY6Z9KZTAghhLBgkqiFEEIICyaJuhzs7Ox49913sbOzM3coVYacs/KTc1Z+cs7KT85Z+ZnrnMk9aiGEEMKCSYtaCCGEsGCSqIUQQggLJolaCCGEsGCSqMth+vTp1KxZE3t7e1q1asW2bdvMHZLFGj9+PFFRUTg7O+Pj40OPHj1ISEgwd1hVxkcffYRGo2H48OHmDsWipaSk8H//9394enri4OBAZGQkO3bsMHdYFqukpIR33nmHWrVq4eDgQJ06dXjvvfeQrkqm1q1bR7du3QgICECj0bBo0SKT9UopRo8ejb+/Pw4ODsTGxnLkyJFKi0cS9U368ccfGTFiBO+++y67du2icePGdOrUifT0dHOHZpHWrl3L4MGD2bJlC3FxcRQVFdGxY0dyc3PNHZrF2759O1999RWNGjUydygW7eLFi8TExGBjY8OyZcs4cOAAn3zyCe7u7uYOzWJNmDCBGTNm8Pnnn3Pw4EEmTJjAxIkTmTZtmrlDsyi5ubk0btyY6dOnX3P9xIkTmTp1Kl9++SVbt27F0dGRTp06kZ+fXzkBKXFTWrZsqQYPHmx8X1JSogICAtT48ePNGFXVkZ6ergC1du1ac4di0bKzs1VoaKiKi4tT7dq1Uy+99JK5Q7JYI0eOVG3atDF3GFVK165d1XPPPWdS9sgjj6g+ffqYKSLLB6iFCxca3+v1euXn56c+/vhjY1lGRoays7NT8+bNq5QYpEV9EwoLC9m5cyexsbHGMq1WS2xsLJs3bzZjZFVHZmYmAB4eHmaOxLINHjyYrl27mvysiWtbvHgxLVq04LHHHsPHx4emTZvy9ddfmzssi9a6dWtWrVrF4cOHAdizZw8bNmygS5cuZo6s6khMTCQ1NdXkd9TV1ZVWrVpVWj6o9rNnVYRz585RUlKCr6+vSbmvry+HDh0yU1RVh16vZ/jw4cTExNCwYUNzh2Ox5s+fz65du9i+fbu5Q6kSjh8/zowZMxgxYgRvvfUW27dvZ9iwYdja2tK3b19zh2eR3njjDbKysggLC8PKyoqSkhI++OAD+vTpY+7QqozU1FSAa+aDy+sqmiRqUekGDx7M/v372bBhg7lDsVjJycm89NJLxMXFYW9vb+5wqgS9Xk+LFi348MMPAWjatCn79+/nyy+/lER9HT/99BNz5sxh7ty5NGjQgPj4eIYPH05AQICcMwsml75vgpeXF1ZWVsa5rS9LS0vDz8/PTFFVDUOGDGHJkiWsXr2awMBAc4djsXbu3El6ejrNmjXD2toaa2tr1q5dy9SpU7G2tqakpMTcIVocf39/IiIiTMrCw8NJSkoyU0SW77XXXuONN97giSeeIDIykqeffpqXX36Z8ePHmzu0KuPy3/w7mQ8kUd8EW1tbmjdvzqpVq4xler2eVatWER0dbcbILJdSiiFDhrBw4UL++usvatWqZe6QLFqHDh3Yt28f8fHxxqVFixb06dOH+Ph4rKyszB2ixYmJiblqyN/hw4cJCQkxU0SWLy8vD63W9M++lZUVer3eTBFVPbVq1cLPz88kH2RlZbF169ZKywdy6fsmjRgxgr59+9KiRQtatmzJlClTyM3N5dlnnzV3aBZp8ODBzJ07l99++w1nZ2fjvRtXV1ccHBzMHJ3lcXZ2vur+vaOjI56ennJf/zpefvllWrduzYcffkjv3r3Ztm0bM2fOZObMmeYOzWJ169aNDz74gODgYBo0aMDu3buZPHkyzz33nLlDsyg5OTkcPXrU+D4xMZH4+Hg8PDwIDg5m+PDhvP/++4SGhlKrVi3eeecdAgIC6NGjR+UEVCl9yaupadOmqeDgYGVra6tatmyptmzZYu6QLBZwzWXWrFnmDq3KkOFZ/+73339XDRs2VHZ2diosLEzNnDnT3CFZtKysLPXSSy+p4OBgZW9vr2rXrq3efvttVVBQYO7QLMrq1auv+ferb9++SinDEK133nlH+fr6Kjs7O9WhQweVkJBQafHI7FlCCCGEBZN71EIIIYQFk0QthBBCWDBJ1EIIIYQFk0QthBBCWDBJ1EIIIYQFk0QthBBCWDBJ1EIIIYQFk0QthBBCWDBJ1EKICqfRaFi0aJG5wxCiWpBELUQ1069fPzQazVVL586dzR2aEOIWyKQcQlRDnTt3ZtasWSZldnZ2ZopGCHE7pEUtRDVkZ2eHn5+fyeLu7g4YLkvPmDGDLl264ODgQO3atfnll19Mtt+3bx/t27fHwcEBT09PBgwYQE5Ojkmd7777jgYNGmBnZ4e/vz9DhgwxWX/u3Dl69uyJTqcjNDSUxYsXG9ddvHiRPn364O3tjYODA6GhoVd9sRBCGEiiFuIu9M4779CrVy/27NlDnz59eOKJJzh48CAAubm5dOrUCXd3d7Zv387PP//MypUrTRLxjBkzGDx4MAMGDGDfvn0sXryYunXrmhxj7Nix9O7dm7179/Lggw/Sp08fLly4YDz+gQMHWLZsGQcPHmTGjBl4eXnduRMgRFVSafNyCSHMom/fvsrKyko5OjqaLB988IFSyjAF6YsvvmiyTatWrdTAgQOVUkrNnDlTubu7q5ycHOP6P/74Q2m1WpWamqqUUiogIEC9/fbb140BUKNGjTK+z8nJUYBatmyZUkqpbt26qWeffbZiPrAQ1ZzcoxaiGrr//vuZMWOGSZmHh4fxdXR0tMm66Oho4uPjATh48CCNGzfG0dHRuD4mJga9Xk9CQgIajYbTp0/ToUOHG8bQqFEj42tHR0dcXFxIT08HYODAgfTq1Ytdu3bRsWNHevToQevWrW/pswpR3UmiFqIacnR0vOpSdEVxcHC4qXo2NjYm7zUaDXq9HoAuXbpw8uRJli5dSlxcHB06dGDw4MFMmjSpwuMVoqqTe9RC3IW2bNly1fvw8HAAwsPD2bNnD7m5ucb1GzduRKvVUr9+fZydnalZsyarVq26rRi8vb3p27cvP/zwA1OmTGHmzJm3tT8hqitpUQtRDRUUFJCammpSZm1tbeyw9fPPP9OiRQvatGnDnDlz2LZtG99++y0Affr04d1336Vv376MGTOGs2fPMnToUJ5++ml8fX0BGDNmDC+++CI+Pj506dKF7OxsNm7cyNChQ28qvtGjR9O8eXMaNGhAQUEBS5YsMX5REEKYkkQtRDW0fPly/P39Tcrq16/PoUOHAEOP7Pnz5zNo0CD8/f2ZN28eERERAOh0OlasWMFLL71EVFQUOp2OXr16MXnyZOO++vbtS35+Pp9++imvvvoqXl5ePProozcdn62tLW+++SYnTpzAwcGBtm3bMn/+/Ar45EJUPxqllDJ3EEKIO0ej0bBw4UJ69Ohh7lCEEDdB7lELIYQQFkwStRBCCGHB5B61EHcZudslRNUiLWohhBDCgkmiFkIIISyYJGohhBDCgkmiFkIIISyYJGohhBDCgkmiFkIIISyYJGohhBDCgkmiFkIIISyYJGohhBDCgv0/IckgnHGxc9YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator  #画图 损失率\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb93c99",
   "metadata": {},
   "source": [
    "- 从上面的结果来看，模型在训练开始时生成了无法理解的词语字符串，而在训练接近尾声时，已经能够生成基本符合语法的句子。\n",
    "- 然而，通过观察训练集和验证集的损失值，我们可以看出模型开始出现**过拟合**。\n",
    "- 如果检查模型在训练后期生成的部分文本，会发现其中一些内容是直接从训练集中逐字复制的——模型仅仅是记住了训练数据。\n",
    "\n",
    "- 稍后我们将介绍一些解码策略，可以在一定程度上减轻mitigate这种记忆化现象。\n",
    "- 需要注意的是，之所以会发生过拟合，是因为我们使用的训练集非常小，且多次重复迭代。\n",
    "    - 此次的语言模型训练主要是为了教学目的，目标是让我们看到模型能够学习生成连贯的文本。\n",
    "    - 相比于花费数周或数月时间在大量昂贵的硬件上训练模型，我们稍后会加载预训练的权重。\n",
    "    \n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462d735",
   "metadata": {},
   "source": [
    "## 5.3 Decoding strategies to control randomness\n",
    "- 对于像上文训练的 GPT 这样相对小的 LLM（大型语言模型），推理过程相对便宜，因此即使在训练时使用了 GPU，在推理时也不需要用 GPU。\n",
    "- generate_text_simple 函数 依次上层一个token/word，依据最大可能性得分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93f21095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()  #评估模式\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")   #分词器\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bfa46",
   "metadata": {},
   "source": [
    "- 即使我们多次执行上面的 generate_text_simple 函数，LLM 也会生成相同的输出。\n",
    "- 我们现在介绍两种被称为解码策略的概念，用于修改 generate_text_simple 函数：温度缩放 和 Top-k 采样 **temperature scaling and top-k sampling。**\n",
    "- 这些策略可以让模型控制生成文本的随机性和多样性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8760dd",
   "metadata": {},
   "source": [
    "### 5.3.1 Temperature scaling\n",
    "- 在之前，我们总是使用 torch.argmax 选取概率最高的 token 作为下一个 token。\n",
    "- 为了增加变化，我们可以使用 torch.multinomial(probs, num_samples=1) 从概率分布中进行采样。\n",
    "- 在这里，每个索引被选中的概率对应于输入张量中的概率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f852d",
   "metadata": {},
   "source": [
    "- 以下是生成下一个 token 的简单回顾，假设一个非常小的词汇表以便于说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc68b490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "## inverse_vocab 将 vocab 中的键值对反转，以便通过 token id 查找对应的单词\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "## 使用 softmax 计算每个 token 的概率分布\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "## 找到概率最大的 token 的索引\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed9b71e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()  #对概率进行采样\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a23f474",
   "metadata": {},
   "source": [
    "- 为了避免总是选择最可能的 token，我们可以通过 torch.multinomial(probas, num_samples=1) 从 softmax 分布中进行采样，决定下一个 token。\n",
    "- 为了便于说明，假设我们使用原始的 softmax 概率对下一个 token 进行 1000 次采样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b598475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility可复现\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406a363",
   "metadata": {},
   "source": [
    "- 我们可以通过一种称为温度缩放 temperature scaling的概念来控制分布和选择过程，指的是将 **logits 除以一个大于 0 的数值temperature**。\n",
    "- temperature大于 1 时，在应用 softmax 后，token 概率会更均匀分布；temperature小于 1 时，在应用 softmax 后，概率分布会更自信（更陡峭或尖峰）--会一直选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a33942e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature           #除以一个正数\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence（变大了）, and lower confidence变小\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aec32c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting画图\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe2a0b",
   "metadata": {},
   "source": [
    "- 可以看到，将温度设置为 0.1 重新缩放后，概率分布更尖锐，接近 torch.argmax，以致于几乎总是选择最可能的单词。\n",
    "- 而温度为 5 时，重新缩放后的概率更趋向于均匀分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62b3805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])   #temperature=0.1 变大，很尖锐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c009735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])  #temperature=5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0cc24e",
   "metadata": {},
   "source": [
    "- 假设 LLM 的输入为“every effort moves you”，在上述方法下，有时会生成像“every effort moves you pizza”这样不合逻辑的文本，大约在 3.2% 的情况下（1000 次中出现了 32 次）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4120b7d",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k  sampling\n",
    "- 为了能够在提高输出多样性（higher temperature）的同时减少生成不合逻辑句子的可能性，我们可以将采样限制在前 k 个最可能的 token 中：\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/topk.webp\" width=500px>\n",
    "- （请注意，此图中的数字已简化为小数点后两位，以减少视觉干扰。Softmax 行中的值应加起来为 1.0。）\n",
    "\n",
    "代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8504dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "#            位置 index索引\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efc3f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(  #条件选择函数\n",
    "    #在 condition真的位置将 input 的值赋给输出张量，而在 condition假（即不符合条件）的地方将 other 的值赋给输出张量。\n",
    "    #top_logits[-1]：前 K 个 token 的最低 logit 值\n",
    "    #如果不是top k里面的，执行input那句\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),       #其他的变成负无穷\n",
    "    other=next_token_logits  #如果是top k里面的，保持原来的值\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fab8c8",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "> 作为选择，更有效一点的方式是：\n",
    "> ```python\n",
    "> new_logits = torch.full_like( # create tensor containing -inf values\n",
    ">    next_token_logits, -torch.inf\n",
    ">)   \n",
    "> new_logits[top_pos] = next_token_logits[top_pos] # copy top k values into the -inf tensor\n",
    "> ```\n",
    "> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2206d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)  #对前K个单独归一化\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe902c18",
   "metadata": {},
   "source": [
    "### 5.3.3 修改Modify文本生成函数 \n",
    "- 前两节介绍了temperature sampling and top-k sampling\n",
    "- 让我们使用这两个概念来修改先前用于生成文本的 generate_simple 函数，创建一个新的 generate 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d43339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eos_id: 终止 token 的索引，遇到此 token 时停止生成\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():  #不计算梯度\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] ## 仅关注最后一个时间步的 logits\n",
    "\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a5fd11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to go a hint a littleoms he painted with a single enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb7c94",
   "metadata": {},
   "source": [
    "## 5.4 Loading and saving model weights in PyTorch\n",
    "- 训练的计算成本很高，保存和加载LLM权重很重要\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-3.webp\" width=400px>\n",
    "- 在PyTorch里推荐的方式是state_dict，通过torch.save函数的 .state_dict()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08ce5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8a88b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#然后我们可以按如下方式将模型权重加载到一个新的 GPTModel 实例中\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();   #评估模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2c12c",
   "metadata": {},
   "source": [
    "- 通常，我们会使用自适应优化器adaptive optimizers，如 Adam 或 AdamW，而不是常规的 SGD 来训练大型语言模型（LLM）。\n",
    "- adaptive optimizers会为每个模型权重存储额外的参数，因此在计划稍后继续预训练时，保存这些参数也是有意义的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c7001bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acac4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载检查点文件 weights_only=True 表示只加载权重信息\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "#从 checkpoint 中提取模型的权重字典，并用 load_state_dict 加载到新创建的 model 中。\n",
    "#\"model_state_dict\" 键包含模型中所有参数的状态，包括权重和偏置，使模型恢复到保存的训练状态。\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "#创建一个新的 AdamW 优化器实例，用于模型的训练。\n",
    "#设定学习率 lr=0.0005 和权重衰减 weight_decay=0.1。\n",
    "#model.parameters() 会将模型的所有参数传递给优化器，使其能在梯度下降过程中更新这些参数。\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "#恢复优化器状态 \n",
    "#\"optimizer_state_dict\" 中包含优化器的动量、学习率调度器等状态信息，可以让训练在上次中断的地方继续。\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "#变成训练状态\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4a3d1",
   "metadata": {},
   "source": [
    "## 5.5 从 OpenAI 加载预训练权重\n",
    "- 之前，我们仅使用一个非常短的小故事书籍进行了小型 GPT-2 模型的教育性训练。感兴趣的读者还可以在 ../03_bonus_pretraining_on_gutenberg 中找到完整的 Project Gutenberg 图书集上的更长的预训练运行记录。\n",
    "\n",
    "- 幸运的是，我们不需要花费数万美元甚至更多来在大型预训练语料库上训练模型，而是可以加载 OpenAI 提供的预训练权重。\n",
    "- 有关从 Hugging Face Hub 加载权重的替代方法，请参阅 ../02_alternative_weight_loading。\n",
    "\n",
    "- 首先，一些样板代码boilerplate code用于从 OpenAI 下载文件并将权重加载到 Python 中。- - 由于 OpenAI 使用了 TensorFlow，因此我们需要安装并使用 TensorFlow 来加载这些权重；tqdm 是一个进度条库。\n",
    "- 取消注释并运行下一个单元格来安装所需的库。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c42b512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\anaconda\\envs\\llms\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\llms\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in d:\\anaconda\\envs\\llms\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.3)\n",
      "Requirement already satisfied: namex in d:\\anaconda\\envs\\llms\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\anaconda\\envs\\llms\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\envs\\llms\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a007a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "tqdm version: 4.66.6\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf60620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f047166",
   "metadata": {},
   "source": [
    "- 接着，我们可以按如下方式下载参数为 124M 的模型权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2586fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1de2c3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3aa62f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "053c7144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f789d48",
   "metadata": {},
   "source": [
    "- 此外，\"355M\"、\"774M\" 和 \"1558M\" 也是受支持的 model_size 参数。下图总结了这些不同大小模型之间的差异：\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-sizes.webp?timestamp=123\" width=500px>\n",
    "\n",
    "- 在上面的操作中，我们将 124M 的 GPT-2 模型权重加载到了 Python 中，但是我们仍然需要将它们传递到我们的 GPTModel 实例中。\n",
    "\n",
    "- 首先，我们初始化一个新的 GPTModel 实例。\n",
    "\n",
    "- 接下来要做的是将 OpenAI 的权重分配给 GPTModel 实例中相应的权重张量。\n",
    "\n",
    "- 需要注意的是，原始的 GPT 模型在multi-head attention module中对查询、键和值矩阵的线性层进行了带偏置向量bias vectors的初始化，这在我们实现中并不是必需的，也不被推荐。然而，为了正确加载权重，我们也需要在实现中通过将 qkv_bias 设置为 True 来启用它们。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5482aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "#通过字典配置不同的 GPT-2 模型参数\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "#将 GPT-2 小模型（124M参数）的基础配置复制到 NEW_CONFIG 中，以便在此基础上添加特定的模型参数。\n",
    "#.copy() 确保原始 GPT_CONFIG_124M 的内容不会被修改。\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()   #复制设置\n",
    "# 从字典 model_configs 中选择所需模型\n",
    "#并更新到 NEW_CONFIG 中。\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "#手动添加了额外配置，\n",
    "#context_length：设置上下文长度为 1024，表示模型可以处理的最大序列长度。\n",
    "#qkv_bias：设置多头注意力层中的 Query、Key、Value 线性层带有偏置项。\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "\n",
    "#实例化模型，使用NEW_CONFIG配置\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();#评估模式：禁用 dropout 等训练特定的操作，使模型适用于推理任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e71af46",
   "metadata": {},
   "source": [
    "- 下一个任务是将OpenAI weights分配给GPTModel instance对应的权重张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb4376da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9934954",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 引入numpy库，提供矩阵运算支持\n",
    "import numpy as np\n",
    "\n",
    "#将预训练好的模型权重 params 加载到 GPT 模型实例 gpt 中\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    #加载embedding层weight\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    #加载每个 Transformer Block 的权重\n",
    "    #params[\"blocks\"] 是一个包含多层 Transformer 的字典。\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        \n",
    "        #加载自注意力层的 Query、Key、Value 权重和偏置\n",
    "        ## 将 c_attn 层的权重分为 Q, K, V 权重\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        # # 将Q、K、V权重分配给GPT模型中的相应层\n",
    "        #assign() 函数用于将这些值（经过转置）赋值给 gpt 中相应的 Query、Key 和 Value 层的权重。\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        #加载自注意力层的偏置项\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        #加载自注意力层的输出投影层权重和偏置\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(  #输出投影层\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(  #偏置\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        #加载 FeedForward 层的权重和偏置\n",
    "        #分别加载多层感知器 (MLP) 中前馈网络的权重和偏置，赋值到 GPT 的 ff.layers[0] 和 ff.layers[2] 中。\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        #加载 LayerNorm 层的缩放和偏移参数\n",
    "        #加载每个 Transformer Block 中的 LayerNorm 层的 scale 和 shift 参数，赋值到 GPT 的 norm1 和 norm2 中。\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    #加载 GPT 的最终层权重和偏置\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "#调用函数加载权重并将模型移动到设备  \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491c21c",
   "metadata": {},
   "source": [
    "- 如果模型加载正确，我们可以使用之前的 generate 函数生成新的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb79f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,   #每次只在概率前50的token之中选择\n",
    "    temperature=1.5   #更温和\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e78e6d",
   "metadata": {},
   "source": [
    "- 我们知道模型权重加载正确，因为模型可以生成连贯的文本；如果我们在加载过程中有任何小的错误，模型将无法做到这一点。\n",
    "\n",
    "- 有关从 Hugging Face Hub 加载权重的替代方法，请参阅 ../02_alternative_weight_loading。\n",
    "\n",
    "- 如果您有兴趣了解 GPT 架构与 Llama 架构（由 Meta AI 开发的热门 LLM）之间的比较，请参阅 ../07_gpt_to_llama 中的附加内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d36dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
