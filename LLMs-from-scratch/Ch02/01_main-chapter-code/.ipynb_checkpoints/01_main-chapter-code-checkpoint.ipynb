{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f86ea49",
   "metadata": {},
   "source": [
    "This chapter covers data preparation and sampling to get input data \"ready\" for the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5416587a",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0daf479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu121\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "#importlib.metadata 是 Python 的标准库模块之一，\n",
    "#用于访问安装在 Python 环境中的软件包的元数据（如版本信息、作者信息等）\n",
    "#version() 函数用于获取指定软件包的版本号。\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "\n",
    "\n",
    "#tiktoken 是一个处理 token 的库，特别是用于 OpenAI 的 API 中 GPT 系列模型的分词。\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1de0e",
   "metadata": {},
   "source": [
    "# <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e00975",
   "metadata": {},
   "source": [
    "## 2.2 分词：将文本转化为token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84a718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load raw text we want to work with\n",
    "\n",
    "#os 模块用于与操作系统交互，比如检查文件和目录的存在性。\n",
    "#urllib.request 模块用于发送 HTTP 请求，这里用来下载文件。\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "#如果path中不存在，就去github上下载\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"    #指定文件的本地保存路径，也就是当前目录下的 the-verdict.txt。\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecba9c8",
   "metadata": {},
   "source": [
    "打开要处理的raw text并进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db4dae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "#用只读模式打开文件，UTF-8 是一种通用的字符编码格式，用于确保不同语言字符的正确读取。\n",
    "#with 语句用于上下文管理。它会在代码块结束后自动关闭文件，避免资源泄漏。\n",
    "\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "#计算 raw_text 的字符总数，包含所有的空格和换行符。\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb4e8b",
   "metadata": {},
   "source": [
    "目的：将文本分词转化成token并且转化为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2e374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "#导入Python的re模块，提供对正则表达式的支持，能进行字符串匹配和操作。\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "\n",
    "#r前缀表示这是一个原始字符串（raw string）\n",
    "#\\s表示任何空白字符：空格、制表符（tab）、换行符等\n",
    "#()用于捕获组，意味着分割时会保留空白字符（分割符）\n",
    "\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27ee83",
   "metadata": {},
   "source": [
    "上面的例子是按照空白字符whitespaces进行分割，下面修改正则表达式regular expression，不仅可以按照空白字符（空格、制表符等）来分割文本，还可以按逗号（,）和句号（.）进行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260f624d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f290ac",
   "metadata": {},
   "source": [
    "1.删除空格字符串\n",
    "2.可以按逗号（,）和句号（.）等其他标点符号进行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28e8edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "#实例文本\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "#|表示“或”，即匹配这些符号中的任意一个。\n",
    "#要作为分隔符的标点符号, . : ; ? ! 等 --代表两个连字符\n",
    "#\\s表示任何空白字符（如空格、制表符等）\n",
    "#()用于捕获这些分隔符，这样分隔符在结果中也会被保留。\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "#item.strip()：去掉每个item前后的空白字符。\n",
    "#if item.strip()：只保留非空的字符串。\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8534240",
   "metadata": {},
   "source": [
    "对生数据tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24369009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "#打印前三十个转化出来的token\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c4fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "#打印token的数量\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49bc98",
   "metadata": {},
   "source": [
    "## 2.3将token转化为token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318feb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "#建立一个包括所有词汇的词汇表\n",
    "\n",
    "#将preprocessed列表转换为一个集合（set），集合会自动去重，确保每个单词只出现一次。\n",
    "#sorted(...)：对集合进行排序，返回一个排序后的列表all_words，包含了所有唯一的单词。\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "#计算all_words列表中元素的数量，即唯一单词的个数，赋值给vocab_size\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4935513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enumerate(all_words)：这个函数会生成一个可迭代的索引-元素对 integer-token\n",
    "#vocab = {token: integer for ...}：字典推导式（dictionary comprehension）将每个单词（token）映射到它的索引（integer）\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f563ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "#vocab.items()：返回词汇表中每个键值对的可迭代视图\n",
    "#enumerate(...)：为每个词汇对提供索引\n",
    "#打印前50个token-integer\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f92f72",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab04b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立完表之后再转化为tokenID，放在tokenizer class\n",
    "\n",
    "#简单分词器类：可以将文本编码为整数ID，并能够将整数ID解码回文本。\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    \n",
    "    #初始化方法，接受一个词汇表 vocab\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab                            #用于字符串映射到整数\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}   #i整数 s字符串。将整数映射回字符串\n",
    "    \n",
    "    #编码方法：将输入的文本编码为整数ID （也就是tokenize）\n",
    "    def encode(self, text):\n",
    "        \n",
    "        #将文本分割为单词和标点符号\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                 \n",
    "        #移除分割后前后字符串空白，过滤空字符串\n",
    "        preprocessed = [\n",
    "          item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        #使用词汇表将每个单词转换为对应的整数ID。\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "\n",
    "        \n",
    "    #解码方法：将整数ID解码回文本\n",
    "    def decode(self, ids):\n",
    "        #将每个整数ID转换为对应的字符串，并使用空格连接它们。\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        # 使用正则表达式去除字符串中标点符号前的空格。\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddeff2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "#测试样例\n",
    "#编码encode\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18f3168a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#编码decode回去\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9119850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#套娃\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3fee2",
   "metadata": {},
   "source": [
    "## 2.4 加入特殊的文本token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ae67a",
   "metadata": {},
   "source": [
    "- 特殊标记：\n",
    "    - 1.[BOS] (Beginning of Sequence)：表示文本的开始，帮助模型理解输入的起始位置。\n",
    "    - 2.[EOS] (End of Sequence)：文本的结束。常用于将多个不相关的文本连接在一起，例如不同的维基百科文章或书籍。它指示模型何时停止生成文本。\n",
    "    - 3.[PAD] (Padding)：在使用批处理（batch）训练时，输入文本的长度可能不同。使用填充标记可以将较短的文本扩展到与最长文本相同的长度，从而形成统一的输入形状。这在训练时很重要，因为深度学习模型通常需要固定大小的输入。\n",
    "    - 4.[UNK] (Unknown)：词汇表中未包含的单词。在某些分词器中，当遇到无法识别的词时，会使用此标记。\n",
    "\n",
    "- GPT-2只使用一个特定的标记 <|endoftext|>\n",
    "- 类似于 [EOS] 标记，用于指示文本的结束。\n",
    "- 也用作填充标记，因为在训练过程中，使用掩码（mask）来处理批量输入时，模型不会关注填充的标记，因此填充的具体内容并不重要。\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"500px\">\n",
    "\n",
    "#### BPE 分词器\n",
    "- GPT-2 采用字节对编码（BPE）分词器，而不是使用 [UNK] 标记处理未在词汇表中的单词。BPE 将单词分解为子词单元，从而更灵活地处理新词或组合词。\n",
    "- 应用\n",
    "    - 在独立文本之间使用 <|endoftext|> 标记：该标记可以帮助模型理解何时开始处理新的文本段落，从而更好地管理上下文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2eae1e",
   "metadata": {},
   "source": [
    "- 某个token没出现在词汇表中：\"<|unk|>\"来表示未知的token\n",
    "- \"<|endoftext|>\" 表示文本的结束，或者训练集中多篇文章的分割点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f0d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set去重 list转换为列表以便排序 sorted进行词典序排序\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "\n",
    "#extend 将两个特殊标记添加到all_tokens列表末尾\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "#为all_tokens的每一个token标记从0开始生成索引\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "076263a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db648e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "#将元组键值对（token，integer）转化为list，遍历得到最后五个元素\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c55712",
   "metadata": {},
   "source": [
    "调整tokenizer：让模型学习什么时候使用<unk> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17d8a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab   #vocab是一个字典 ：token字符串转化为整数\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "  \n",
    "    #编码\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)  #分割字符串\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]  #去掉空字符串和前后的空格\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int            #如果vocab中有则保留\n",
    "            else \"<|unk|>\" for item in preprocessed    #如果没有用<|unk|>代替\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "      \n",
    "    #解码\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eda1f15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "#使用示例\n",
    "\n",
    "#初始化分词器\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "#使用 join 方法，将 text1 和 text2 连接起来，之间插入 <|endoftext|>。\n",
    "#这个标记常用于指示文本段落的分隔，尤其在训练语言模型时有助于区分不同的上下文。\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ad714df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a6722c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6efdf",
   "metadata": {},
   "source": [
    "## 2.5 BytePair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d32a5",
   "metadata": {},
   "source": [
    "- BPE通过将不常见的词分解成更小的子词单元或字符，使得模型能够理解这些未知的词汇。\n",
    "\n",
    "- 例如，如果 GPT-2 的词汇表中没有 \"unfamiliarword\" 这个词，它可能会将其分解为 [\"unfam\", \"iliar\", \"word\"]，或者根据其训练的 BPE 合并规则进行其他分解。\n",
    "\n",
    "    - 1.原始BPE分词器\n",
    "    - 2.OpenAI 的开源 tiktoken 库来实现 BPE 分词器。核心算法用 Rust 语言实现，提高计算性能。在大规模文本上处理速度更快，约5倍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc820d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "#导包并验证\n",
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f30e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入BPE\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f79a6fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "#导入文本，允许使用<|endoftext|>，打印token ID\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6308da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b87b766",
   "metadata": {},
   "source": [
    "## 2.6 滑动窗口进行数据采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9a033",
   "metadata": {},
   "source": [
    "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22f1f5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "#导入待处理的文本进行编码\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2bfe0",
   "metadata": {},
   "source": [
    "- For each text chunk, we want the inputs and targets\n",
    "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4902ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#从第50个到结尾\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8b00ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "#尝试拿出来一组滑动窗口\n",
    "#一次右移一个位置，一次使用4个单词/字符作为输入来预测下一个\n",
    "context_size = 4\n",
    "\n",
    "#提取前4个元素，作为模型的输入  索引0-3\n",
    "#提取第2-第5元素              索引1-4\n",
    "#这将作为模型的目标输出\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "#输出的是x y token ID\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "#预测模式：输出tokenID\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)\n",
    "    \n",
    "\n",
    "#输出解码后的    \n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0be50",
   "metadata": {},
   "source": [
    "数据加载器：输入数据集上迭代并返回移位1的输入和目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c52aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1e7eb",
   "metadata": {},
   "source": [
    "- We use a sliding window approach, changing the position by +1:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">\n",
    "- Create dataset and dataloader that extract chunks from the input text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99308478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset：结构化的方法来存储和访问数据，使得我们可以轻松地处理和迭代大型数据集。\n",
    "# DataLoader 可以实现：批处理、打乱数据、并行加载\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#这个类继承DataSet\n",
    "class GPTDatasetV1(Dataset):\n",
    "    #                        分词器                窗口滑动的步幅，用于生成重叠序列\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #使用滑动窗口将文本切分为部分重叠序列 of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            #torch.tensor 将python列表转换为张量，当前窗口内的输入和下一个目标。\n",
    "            #实现数据加载的重要步骤，将数据存储为张量，可以利用 PyTorch 的高效计算能力和 GPU 加速。\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8af6e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size每个批次的大小、每个序列的最大长度、stride滑动窗口的步幅 控制重叠部分的大小、\n",
    "#shuffle：布尔值，决定是否在每个 epoch 开始时随机打乱数据，默认值为 True。\n",
    "#drop_last：布尔值，决定在批次不足时是否丢弃最后一个批次，默认值为 True。\n",
    "#num_workers：用于数据加载的子进程数量，默认值为 0，表示在主进程中加载数据。\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfc7dad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "#测试一下！\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "    \n",
    "dataloader = create_dataloader_v1(\n",
    "    #                           滑动窗口每次移动一个单位  不打乱数据顺序\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "#将数据加载器转换为迭代器，以便逐批次获取数据。\n",
    "data_iter = iter(dataloader)\n",
    "#获取第一个批次的数据。\n",
    "first_batch = next(data_iter)\n",
    "#打印第一个批次的内容。输出将包含输入 IDs 和目标 IDs，通常是张量格式。\n",
    "print(first_batch)\n",
    "\n",
    "\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b516fdf",
   "metadata": {},
   "source": [
    "- An example using stride equal to the context length (here: 4) as shown below:\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">\n",
    "- We can also create batched outputs\n",
    "- Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "352d07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a7a2ee",
   "metadata": {},
   "source": [
    "## 2.7 token ID->embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c99ba4",
   "metadata": {},
   "source": [
    "- 嵌入层（embedding layer）实现.\n",
    "- 嵌入层是神经网络中的一种特殊层，用于将离散的标记（例如词汇表中的 ID）映射到连续的向量空间。每个标记都有一个对应的向量表示，这些向量可以捕捉到标记之间的语义关系。\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bad03288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#假设tokenization之后的输入是\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "#简单起见，假设只有一个6个词的vocabulary，向量空间是3维的\n",
    "#6*3的矩阵（词表大小*维度）\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68ae14c6",
   "metadata": {},
   "source": [
    "- 过程：\n",
    "- 创建嵌入矩阵：初始化一个嵌入层。\n",
    "- 输入标记 ID：准备要转换的标记 ID。\n",
    "- 查找嵌入向量：从嵌入层中获取对应的向量。\n",
    "- 优化：在训练过程中更新嵌入层的权重。\n",
    "\n",
    "嵌入层的工作原理类似于一热编码加上全连接层的矩阵乘法。嵌入层的权重在模型训练过程中可以通过反向传播进行优化，这意味着它可以学习到不同标记之间的相对关系和语义信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ca1fc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#把ID为3的token转化为三维向量\n",
    "\n",
    "#调用嵌入层查ID3的嵌入向量    创建了一个张量，\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "#这是第四行的，索引从0开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b396035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76d46b",
   "metadata": {},
   "source": [
    "- An embedding layer is essentially a look-up operation:\n",
    "- <img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abfff5",
   "metadata": {},
   "source": [
    "## 2.8 为单词位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be511346",
   "metadata": {},
   "source": [
    "- Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"400px\">\n",
    "- Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">\n",
    "- The BytePair encoder has a vocabulary size of 50,257:\n",
    "- Suppose we want to encode the input tokens into a 256-dimensional vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "257f465c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "#BPE的词表size 50257\n",
    "#假设编码到256维\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "#如果batch size=8，每一个batch有4个token，需要8*4*256个张量Tensor\n",
    "\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3398f7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb9211",
   "metadata": {},
   "source": [
    "- GPT-2 的架构中，绝对位置嵌入（absolute position embeddings）用于为输入序列中的每个标记提供位置信息。\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "991e4d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "#创建一个位置嵌入层\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "#输入嵌入  相加得到\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
